import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler  # WeightedRandomSampler: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ìƒ˜í”ŒëŸ¬
from torchvision import transforms, models

from PIL import Image
import numpy as np
import os

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve, roc_curve
from collections import Counter  # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹°
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class StockChartDataset(Dataset):
    def __init__(self, image_files, labels, transform=None):
        self.image_files = image_files
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(label, dtype=torch.float)
# Define the directory containing the images
image_dir = 'C:/Users/Admin/workspace/project3/images/'
image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')]
labels = [int(f.split('-')[-1].split('.')[0]) for f in os.listdir(image_dir) if f.endswith('.jpg')]

# Split the dataset into training, validation, and testing sets
train_files, test_files, train_labels, test_labels = train_test_split(image_files, labels, test_size=0.2, random_state=42, stratify=labels)
train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=42, stratify=train_labels)

# Define transformations for data augmentation and normalization
data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

# Create dataset instances
train_dataset = StockChartDataset(train_files, train_labels, transform=data_transforms['train'])
val_dataset = StockChartDataset(val_files, val_labels, transform=data_transforms['val'])
test_dataset = StockChartDataset(test_files, test_labels, transform=data_transforms['val'])

# Create DataLoader instances
batch_size = 32
# Note: DataLoaders will be created after computing class weights (see below)

print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of validation samples: {len(val_dataset)}")
print(f"Number of testing samples: {len(test_dataset)}")

# ====================================================================
# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„ ë° ëŒ€ì‘
# ====================================================================
# ì£¼ì‹ ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìƒìŠ¹/í•˜ë½ íŒ¨í„´ì˜ ë¶„í¬ê°€ ë¶ˆê· í˜•í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
# ì˜ˆ: ìƒìŠ¹ íŒ¨í„´ 70%, í•˜ë½ íŒ¨í„´ 30%ì™€ ê°™ì€ ë¶ˆê· í˜•ì´ ì¡´ì¬í•  ìˆ˜ ìˆìŒ
# ì´ëŸ¬í•œ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì´ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì— í¸í–¥ë˜ì–´ í•™ìŠµë©ë‹ˆë‹¤.
# ====================================================================

print("\n=== Class Imbalance Analysis ===")

# Counterë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ê³„ì‚°
class_counts = Counter(train_labels)
print(f"Class 0 (Down): {class_counts[0]} samples")
print(f"Class 1 (Up): {class_counts[1]} samples")
total = sum(class_counts.values())
print(f"Class 0 ratio: {class_counts[0]/total*100:.2f}%")
print(f"Class 1 ratio: {class_counts[1]/total*100:.2f}%")

# ====================================================================
# pos_weight ê³„ì‚° (BCEWithLogitsLossìš©)
# ====================================================================
# pos_weightëŠ” ì–‘ì„± í´ë˜ìŠ¤(Class 1)ì˜ ì†ì‹¤ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
# ê³µì‹: pos_weight = (negative samples) / (positive samples)
# ì˜ˆì‹œ: Class 0ì´ 700ê°œ, Class 1ì´ 300ê°œë¼ë©´ pos_weight = 700/300 = 2.33
# ì´ëŠ” Class 1ì˜ ì†ì‹¤ì„ 2.33ë°° ë” ì¤‘ìš”í•˜ê²Œ ì·¨ê¸‰í•˜ì—¬ ë¶ˆê· í˜•ì„ ë³´ì •í•©ë‹ˆë‹¤.
# ====================================================================
pos_weight = torch.tensor([class_counts[0] / class_counts[1]], dtype=torch.float32).to(device)
print(f"\nCalculated pos_weight for BCEWithLogitsLoss: {pos_weight.item():.4f}")
print("â†’ Class 1(Up)ì˜ ì†ì‹¤ì´ {:.2f}ë°° ë” ì¤‘ìš”í•˜ê²Œ ê³„ì‚°ë©ë‹ˆë‹¤.".format(pos_weight.item()))

# ====================================================================
# WeightedRandomSamplerë¥¼ ìœ„í•œ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì˜µì…˜)
# ====================================================================
# WeightedRandomSamplerëŠ” í›ˆë ¨ ì¤‘ ìƒ˜í”Œë§ í™•ë¥ ì„ ì¡°ì •í•˜ì—¬ ë¶ˆê· í˜•ì„ í•´ê²°í•©ë‹ˆë‹¤.
# ê° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ = 1 / (í•´ë‹¹ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜)
# ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œì´ ë” ìì£¼ ì„ íƒë˜ë„ë¡ í•˜ì—¬ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤.
# 
# ì¥ì : ë°°ì¹˜ë§ˆë‹¤ í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ê· í˜•ìˆê²Œ ìœ ì§€ë¨
# ë‹¨ì : ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì´ ì¤‘ë³µ ìƒ˜í”Œë§ë  ìˆ˜ ìˆìŒ (replacement=True)
# ====================================================================
class_weights = {0: 1.0 / class_counts[0], 1: 1.0 / class_counts[1]}
sample_weights = [class_weights[label] for label in train_labels]
sampler = WeightedRandomSampler(
    weights=sample_weights,           # ê° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜
    num_samples=len(sample_weights),  # ì „ì²´ ìƒ˜í”Œ ìˆ˜
    replacement=True                  # ì¤‘ë³µ ìƒ˜í”Œë§ í—ˆìš© (ê· í˜•ì„ ìœ„í•´ í•„ìš”)
)
print(f"\nWeightedRandomSampler created with weights:")
print(f"  - Class 0 (Down): {class_weights[0]:.6f} (ë” ì ì€ í´ë˜ìŠ¤ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ ë†’ìŒ)")
print(f"  - Class 1 (Up): {class_weights[1]:.6f}")
print("=" * 50)

# ====================================================================
# DataLoader ìƒì„± - í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ ì „ëµ ì„ íƒ
# ====================================================================
# í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ë°©ë²• 2ê°€ì§€:
# 
# ã€ì˜µì…˜ 1ã€‘ WeightedRandomSampler ì‚¬ìš©
#   - ìƒ˜í”Œë§ ë‹¨ê³„ì—ì„œ ë¶ˆê· í˜• í•´ê²° (ë°ì´í„° ë ˆë²¨)
#   - ì†Œìˆ˜ í´ë˜ìŠ¤ë¥¼ ë” ìì£¼ ìƒ˜í”Œë§í•˜ì—¬ ë°°ì¹˜ ë‚´ ê· í˜• ìœ ì§€
#   - sampler ì‚¬ìš© ì‹œ shuffleê³¼ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€ (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ)
#   - ì‚¬ìš©ë²•: train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)
# 
# ã€ì˜µì…˜ 2ã€‘ pos_weightë§Œ ì‚¬ìš© (í˜„ì¬ ì„¤ì •)
#   - ì†ì‹¤ í•¨ìˆ˜ ë ˆë²¨ì—ì„œ ë¶ˆê· í˜• í•´ê²°
#   - ì¼ë°˜ì ì¸ shuffle ì‚¬ìš© ê°€ëŠ¥
#   - ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ì†ì‹¤ì„ ë” í¬ê²Œ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì‹œ ë” ì¤‘ìš”í•˜ê²Œ ì·¨ê¸‰
#   - ë” ê°„ë‹¨í•˜ê³  ì¼ë°˜ì ìœ¼ë¡œ íš¨ê³¼ì 
# ====================================================================

# ì˜µì…˜ 1: WeightedRandomSampler ì‚¬ìš© (ì•„ë˜ ì£¼ì„ í•´ì œ)
# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)

# ì˜µì…˜ 2: ì¼ë°˜ shuffle ì‚¬ìš© + pos_weight ì ìš© (í˜„ì¬ ì„¤ì • âœ“)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print("\nâœ“ DataLoader ì„¤ì •: ì¼ë°˜ shuffle + pos_weight ë°©ì‹ ì‚¬ìš©")
print("  (WeightedRandomSamplerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ìœ„ì˜ ì˜µì…˜ 1 ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”)")
print("  ê¶Œì¥: pos_weight ë°©ì‹ì´ ë” ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì…ë‹ˆë‹¤.")

# ====================================================================
# ResNet18 ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸° ì„¤ì •
# ====================================================================
# ì‚¬ì „í•™ìŠµëœ ResNet18ì„ ë¡œë“œí•˜ê³  ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ FC ë ˆì´ì–´ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
# ====================================================================
model = models.resnet18(pretrained=True)

# ëª¨ë“  íŒŒë¼ë¯¸í„° ë™ê²° (ì´ˆê¸° ìƒíƒœ)
for param in model.parameters():
    param.requires_grad = False

# ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ìµœì¢… FC ë ˆì´ì–´ êµì²´
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 1)

model = model.to(device)

print("\n" + "=" * 70)
print("ğŸ“¦ ResNet18 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print("=" * 70)
print(f"âœ“ ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©: ImageNet")
print(f"âœ“ ìµœì¢… ë ˆì´ì–´: {num_ftrs} â†’ 1 (Binary Classification)")
print(f"âœ“ ì´ˆê¸° ìƒíƒœ: ëª¨ë“  ë ˆì´ì–´ ë™ê²° (fcë§Œ í•™ìŠµ ê°€ëŠ¥)")
print("=" * 70)

# ====================================================================
# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ - pos_weightë¥¼ ì ìš©í•œ BCEWithLogitsLoss
# ====================================================================
# BCEWithLogitsLoss: ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜
# - Sigmoid + Binary Cross Entropyë¥¼ ê²°í•©í•˜ì—¬ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì 
# - pos_weight íŒŒë¼ë¯¸í„°: ì–‘ì„± í´ë˜ìŠ¤(Class 1)ì˜ ì†ì‹¤ì— ê°€ì¤‘ì¹˜ ì ìš©
#
# ì‘ë™ ì›ë¦¬:
# - pos_weightê°€ ì—†ì„ ë•Œ: loss = -[y*log(p) + (1-y)*log(1-p)]
# - pos_weight ì ìš© ì‹œ: loss = -[y*pos_weight*log(p) + (1-y)*log(1-p)]
# - Class 1ì˜ ì†ì‹¤ì´ pos_weightë°° ë§Œí¼ ì¦í­ë˜ì–´ ì†Œìˆ˜ í´ë˜ìŠ¤ í•™ìŠµ ê°•í™”
#
# ì˜ˆì‹œ: pos_weight=2.33ì´ë©´ ìƒìŠ¹(Class 1) ì˜ˆì¸¡ ì‹¤íŒ¨ ì‹œ ì†ì‹¤ì´ 2.33ë°°ë¡œ ê³„ì‚°
# ====================================================================
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
print(f"\nâœ“ BCEWithLogitsLoss initialized with pos_weight={pos_weight.item():.4f}")
print(f"  â†’ í´ë˜ìŠ¤ 1(ìƒìŠ¹)ì˜ ì˜¤ë¶„ë¥˜ íŒ¨ë„í‹°ê°€ {pos_weight.item():.2f}ë°° ì¦ê°€í•©ë‹ˆë‹¤.")
print(f"  â†’ ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ì†Œìˆ˜ í´ë˜ìŠ¤ë¥¼ ë” ì˜ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.")

# ====================================================================
# ë‹¨ê³„ì  íŒŒì¸íŠœë‹ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
# ====================================================================
# ì „ì´ í•™ìŠµ(Transfer Learning)ì—ì„œ ë‹¨ê³„ì  ì–¸í”„ë¦¬ì¦ˆëŠ” ë§¤ìš° ì¤‘ìš”í•œ ê¸°ë²•ì…ë‹ˆë‹¤.
# 
# ã€ì™œ ë‹¨ê³„ì ìœ¼ë¡œ í•´ì•¼ í•˜ëŠ”ê°€?ã€‘
# 1. ì‚¬ì „í•™ìŠµëœ ì €ìˆ˜ì¤€ íŠ¹ì§•(edges, textures)ì€ ëŒ€ë¶€ë¶„ ìœ ìš©í•¨
# 2. ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì „ì²´ í•™ìŠµì€ ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ë¥¼ ë§ì¹  ìˆ˜ ìˆìŒ
# 3. ìƒìœ„ ë ˆì´ì–´ë¶€í„° ì ì§„ì ìœ¼ë¡œ í•´ì œí•˜ë©´ ì•ˆì •ì ì¸ í•™ìŠµ ê°€ëŠ¥
# 
# ã€ì°¨ë³„ì  í•™ìŠµë¥ (Discriminative Learning Rate)ã€‘
# - í•˜ìœ„ ë ˆì´ì–´(layer1, layer2): ë³€ê²½ ìµœì†Œí™” (ë§¤ìš° ì‘ì€ lr ë˜ëŠ” frozen)
# - ì¤‘ê°„ ë ˆì´ì–´(layer3): ì•½ê°„ ì¡°ì • (ì‘ì€ lr)
# - ìƒìœ„ ë ˆì´ì–´(layer4): ë„ë©”ì¸ì— ë§ê²Œ ì¡°ì • (ì¤‘ê°„ lr)
# - FC ë ˆì´ì–´: ì™„ì „íˆ ìƒˆë¡œ í•™ìŠµ (í° lr)
# ====================================================================

def get_trainable_params_info(model):
    """
    í˜„ì¬ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì •ë³´ë¥¼ ë°˜í™˜
    
    Returns:
        trainable_params: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        total_params: ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜
    """
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    return trainable_params, total_params

def unfreeze_layer(model, layer_name):
    """
    íŠ¹ì • ë ˆì´ì–´ë¥¼ ì–¸í”„ë¦¬ì¦ˆí•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
    
    Args:
        model: PyTorch ëª¨ë¸
        layer_name: ì–¸í”„ë¦¬ì¦ˆí•  ë ˆì´ì–´ ì´ë¦„ (ì˜ˆ: 'layer4', 'layer3')
    """
    layer = getattr(model, layer_name)
    for param in layer.parameters():
        param.requires_grad = True
    print(f"   âœ“ {layer_name} ì–¸í”„ë¦¬ì¦ˆ ì™„ë£Œ")

# ====================================================================
# ì„ê³„ê°’ íŠœë‹ í•¨ìˆ˜
# ====================================================================
# F1-scoreë¥¼ ìµœëŒ€í™”í•˜ëŠ” ìµœì ì˜ ì„ê³„ê°’ì„ ì°¾ëŠ” í•¨ìˆ˜
# ê¸°ë³¸ 0.5 ëŒ€ì‹  ë°ì´í„°ì— ë§ëŠ” ìµœì  ì„ê³„ê°’ì„ ì°¾ì•„ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.
# ====================================================================
def find_optimal_threshold(model, dataloader, device):
    """
    ê²€ì¦ ë°ì´í„°ì—ì„œ F1-scoreë¥¼ ìµœëŒ€í™”í•˜ëŠ” ìµœì  ì„ê³„ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Args:
        model: í•™ìŠµëœ PyTorch ëª¨ë¸
        dataloader: ê²€ì¦ ë°ì´í„°ë¡œë”
        device: ì—°ì‚° ì¥ì¹˜ (CPU/GPU)
    
    Returns:
        best_threshold: F1-scoreê°€ ìµœëŒ€ì¸ ì„ê³„ê°’
        best_f1: ìµœëŒ€ F1-score ê°’
    """
    model.eval()
    all_probs = []
    all_labels = []
    
    print("\n" + "=" * 70)
    print("ğŸ” ìµœì  ì„ê³„ê°’(Threshold) íƒìƒ‰ ì¤‘...")
    print("=" * 70)
    
    # ê²€ì¦ ë°ì´í„°ì—ì„œ ì˜ˆì¸¡ í™•ë¥  ìˆ˜ì§‘
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            outputs = model(inputs).squeeze()
            probs = torch.sigmoid(outputs)  # logitsë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (0~1 ì‚¬ì´)
            
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    all_probs = np.array(all_probs)
    all_labels = np.array(all_labels)
    
    # Precision-Recall ê³¡ì„ ì—ì„œ ë‹¤ì–‘í•œ ì„ê³„ê°’ê³¼ ê·¸ì— ë”°ë¥¸ precision, recall ê³„ì‚°
    precisions, recalls, thresholds = precision_recall_curve(all_labels, all_probs)
    
    # ê° ì„ê³„ê°’ì— ëŒ€í•œ F1-score ê³„ì‚°
    # F1 = 2 * (precision * recall) / (precision + recall)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    
    # F1-scoreê°€ ìµœëŒ€ì¸ ì„ê³„ê°’ ì°¾ê¸°
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5
    best_f1 = f1_scores[best_idx]
    best_precision = precisions[best_idx]
    best_recall = recalls[best_idx]
    
    # ê¸°ë³¸ 0.5 ì„ê³„ê°’ê³¼ ë¹„êµ
    default_preds = (all_probs > 0.5).astype(int)
    default_f1 = f1_score(all_labels, default_preds)
    
    print(f"\nğŸ“Š ì„ê³„ê°’ íƒìƒ‰ ê²°ê³¼:")
    print(f"   ê¸°ë³¸ ì„ê³„ê°’ (0.5000):")
    print(f"      - F1-score: {default_f1:.4f}")
    print(f"\n   âœ¨ ìµœì  ì„ê³„ê°’ ({best_threshold:.4f}):")
    print(f"      - F1-score: {best_f1:.4f} (â†‘ {(best_f1-default_f1)*100:+.2f}%p)")
    print(f"      - Precision: {best_precision:.4f}")
    print(f"      - Recall: {best_recall:.4f}")
    print(f"\nğŸ’¡ ìµœì  ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ë©´ F1-scoreê°€ ê°œì„ ë©ë‹ˆë‹¤!")
    print("=" * 70)
    
    # ì„ê³„ê°’ vs F1-score ê·¸ë˜í”„ ì €ì¥
    plot_threshold_analysis(thresholds, f1_scores, precisions, recalls, best_threshold, best_f1)
    
    return best_threshold, best_f1

def plot_threshold_analysis(thresholds, f1_scores, precisions, recalls, best_threshold, best_f1):
    """ì„ê³„ê°’ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    plt.figure(figsize=(12, 5))
    
    # F1-score vs Threshold
    plt.subplot(1, 2, 1)
    plt.plot(thresholds, f1_scores[:-1], 'b-', linewidth=2, label='F1-score')
    plt.axvline(x=best_threshold, color='r', linestyle='--', linewidth=2, 
                label=f'Optimal: {best_threshold:.4f}')
    plt.axvline(x=0.5, color='gray', linestyle=':', linewidth=1.5, label='Default: 0.5')
    plt.xlabel('Threshold', fontsize=12)
    plt.ylabel('F1-score', fontsize=12)
    plt.title('F1-score vs Threshold', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Precision & Recall vs Threshold
    plt.subplot(1, 2, 2)
    plt.plot(thresholds, precisions[:-1], 'g-', linewidth=2, label='Precision')
    plt.plot(thresholds, recalls[:-1], 'orange', linewidth=2, label='Recall')
    plt.axvline(x=best_threshold, color='r', linestyle='--', linewidth=2, 
                label=f'Optimal: {best_threshold:.4f}')
    plt.axvline(x=0.5, color='gray', linestyle=':', linewidth=1.5, label='Default: 0.5')
    plt.xlabel('Threshold', fontsize=12)
    plt.ylabel('Score', fontsize=12)
    plt.title('Precision & Recall vs Threshold', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('threshold_analysis.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ“ˆ ì„ê³„ê°’ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥: threshold_analysis.png")
    plt.close()

def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, 
                num_epochs=25, patience=5, min_delta=0.001):
    """
    EarlyStoppingì„ í¬í•¨í•œ í•™ìŠµ í•¨ìˆ˜
    
    Args:
        model: PyTorch ëª¨ë¸
        train_dataloader: í›ˆë ¨ ë°ì´í„°ë¡œë”
        val_dataloader: ê²€ì¦ ë°ì´í„°ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        scheduler: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        num_epochs: ìµœëŒ€ ì—í¬í¬ ìˆ˜
        patience: EarlyStopping patience (ëª‡ epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)
        min_delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰
    
    Returns:
        model: í•™ìŠµëœ ëª¨ë¸
        best_val_loss: ìµœê³  ê²€ì¦ ì†ì‹¤
        best_val_acc: ìµœê³  ê²€ì¦ ì •í™•ë„
    """
    best_model_wts = model.state_dict()
    best_val_acc = 0.0
    best_val_loss = float('inf')
    
    # EarlyStoppingì„ ìœ„í•œ ë³€ìˆ˜
    patience_counter = 0

    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
                dataloader = train_dataloader
            else:
                model.eval()   # Set model to evaluate mode
                dataloader = val_dataloader

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data
            for inputs, labels in dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Zero the parameter gradients
                optimizer.zero_grad()

                # Forward pass
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs).squeeze()  # ëª¨ë¸ ì¶œë ¥ (logits)
                    loss = criterion(outputs, labels)   # pos_weightê°€ ì ìš©ëœ ì†ì‹¤ ê³„ì‚°
                    
                    # ì˜ˆì¸¡: logitsë¥¼ sigmoidë¡œ í™•ë¥ ë¡œ ë³€í™˜ í›„ 0.5 ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜
                    # BCEWithLogitsLossëŠ” ë‚´ë¶€ì ìœ¼ë¡œ sigmoidë¥¼ ì ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ë‹¤ì‹œ ì ìš©
                    preds = torch.sigmoid(outputs) > 0.5

                    # Backward pass + optimize only in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # Statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # Validation phaseì—ì„œ best model ì²´í¬ ë° EarlyStopping
            if phase == 'val':
                # ê°œì„  ì²´í¬ (loss ê¸°ì¤€)
                if epoch_loss < (best_val_loss - min_delta):
                    print(f'   â†’ ê²€ì¦ ì†ì‹¤ ê°œì„ : {best_val_loss:.4f} â†’ {epoch_loss:.4f}')
                    best_val_loss = epoch_loss
                    best_val_acc = epoch_acc
                    best_model_wts = model.state_dict()
                    patience_counter = 0
                else:
                    patience_counter += 1
                    print(f'   â†’ EarlyStopping ì¹´ìš´í„°: {patience_counter}/{patience}')
                
                # EarlyStopping ì²´í¬
                if patience_counter >= patience:
                    print(f'\nâš ï¸  EarlyStopping ë°œë™: {patience} epoch ë™ì•ˆ ê°œì„  ì—†ìŒ')
                    print(f'   ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}, ì •í™•ë„: {best_val_acc:.4f}')
                    model.load_state_dict(best_model_wts)
                    return model, best_val_loss, best_val_acc

        # Step the scheduler after each epoch
        if scheduler is not None:
            scheduler.step()
        print()

    print(f'âœ“ ì „ì²´ {num_epochs} epoch ì™„ë£Œ')
    print(f'  ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}, ì •í™•ë„: {best_val_acc:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_wts)
    return model, best_val_loss, best_val_acc


# ====================================================================
# ë‹¨ê³„ì  íŒŒì¸íŠœë‹ ì „ëµ (Progressive Unfreezing with Adaptive Training)
# ====================================================================
# ã€ì „ëµ ê°œìš”ã€‘
# Transfer Learningì—ì„œ ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•´
# ë ˆì´ì–´ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì–¸í”„ë¦¬ì¦ˆí•˜ë©´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.
#
# ã€ê°œì„ ëœ 3ë‹¨ê³„ ì „ëµ - EarlyStopping & ì¡°ê±´ë¶€ ì‹¤í–‰ã€‘
# Step 1: FC ë ˆì´ì–´ë§Œ í•™ìŠµ (base frozen)
#   - ëª©ì : ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ë§ëŠ” ì¶œë ¥ ë ˆì´ì–´ ì´ˆê¸°í™”
#   - í•™ìŠµë¥ : 1e-3 (ìƒëŒ€ì ìœ¼ë¡œ í° ê°’)
#   - Max Epochs: 20, EarlyStopping patience=5
#   - í•­ìƒ ì‹¤í–‰ (í•„ìˆ˜ ë‹¨ê³„)
#
# Step 2: Layer4 ì–¸í”„ë¦¬ì¦ˆ + Fine-tuning
#   - ëª©ì : ìƒìœ„ íŠ¹ì§•ì„ ë„ë©”ì¸ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •
#   - í•™ìŠµë¥ : layer4=1e-5 (ì‘ìŒ), fc=1e-4 (ì¤‘ê°„)
#   - Max Epochs: 15, EarlyStopping patience=5
#   - í•­ìƒ ì‹¤í–‰
#   - Step 1 ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ ì²´í¬ â†’ Step 3 ì§„í–‰ ì—¬ë¶€ ê²°ì •
#
# Step 3: Layer3ê¹Œì§€ ì–¸í”„ë¦¬ì¦ˆ + Deep Fine-tuning (ì¡°ê±´ë¶€)
#   - ëª©ì : ì¤‘ê°„ ë ˆë²¨ íŠ¹ì§•ê¹Œì§€ ë„ë©”ì¸ íŠ¹í™”
#   - í•™ìŠµë¥ : layer3=5e-6 (ë§¤ìš° ì‘ìŒ), layer4=1e-5, fc=1e-4
#   - Max Epochs: 10, EarlyStopping patience=5
#   - ì¡°ê±´: Step 2ì—ì„œ 0.5% ì´ìƒ ì„±ëŠ¥ í–¥ìƒ ì‹œì—ë§Œ ì‹¤í–‰
#   - í–¥ìƒ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (ê³¼ì í•© ë°©ì§€)
#
# ã€í•™ìŠµë¥  ì„¤ì • ì›ì¹™ã€‘
# - í•˜ìœ„ ë ˆì´ì–´ì¼ìˆ˜ë¡ ì‘ì€ í•™ìŠµë¥  (ì‚¬ì „í•™ìŠµ ì§€ì‹ ë³´ì¡´)
# - ìƒìœ„ ë ˆì´ì–´ì¼ìˆ˜ë¡ í° í•™ìŠµë¥  (íƒœìŠ¤í¬ íŠ¹í™”)
# - FC ë ˆì´ì–´ëŠ” ê°€ì¥ í° í•™ìŠµë¥  (ì™„ì „íˆ ìƒˆë¡œ í•™ìŠµ)
#
# ã€EarlyStopping ì¥ì ã€‘
# - ê³¼ì í•© ë°©ì§€: validation loss ëª¨ë‹ˆí„°ë§
# - íš¨ìœ¨ì„±: ë¶ˆí•„ìš”í•œ epoch ìƒëµ â†’ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•
# - ìë™í™”: ê° ë‹¨ê³„ë§ˆë‹¤ ìµœì  epoch ìë™ ê²°ì •
#
# ã€ì¡°ê±´ë¶€ ì‹¤í–‰ ì¥ì ã€‘
# - ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°
# - ë°ì´í„°ì…‹ íŠ¹ì„±ì— ë§ëŠ” ìœ ì—°í•œ í•™ìŠµ
# - ê³¼ì í•© ìœ„í—˜ ìµœì†Œí™”
# ====================================================================

print("\n" + "=" * 70)
print("ğŸ¯ ë‹¨ê³„ì  íŒŒì¸íŠœë‹ ì „ëµ ì‹œì‘")
print("=" * 70)
print("ğŸ’¡ ì „ëµ: Progressive Unfreezing with Adaptive Training")
print("   - ì‚¬ì „í•™ìŠµ ì§€ì‹ ë³´ì¡´ + ë„ë©”ì¸ íŠ¹í™” í•™ìŠµ")
print("   - EarlyStopping: ê° ë‹¨ê³„ë§ˆë‹¤ ìµœì  epoch ìë™ ê²°ì •")
print("   - ì¡°ê±´ë¶€ ì‹¤í–‰: ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
print("=" * 70)

# ====================================================================
# Step 1: FC ë ˆì´ì–´ë§Œ í•™ìŠµ
# ====================================================================
# ë¨¼ì € FC ë ˆì´ì–´ë§Œ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ë¶„ë¥˜ íƒœìŠ¤í¬ì— ì ì‘ì‹œí‚µë‹ˆë‹¤.
# Base networkëŠ” frozen ìƒíƒœë¡œ ìœ ì§€í•˜ì—¬ ì‚¬ì „í•™ìŠµëœ íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
# ====================================================================
print("\nã€Step 1ã€‘ FC ë ˆì´ì–´ë§Œ í•™ìŠµ (Base Frozen)")
print("-" * 70)

trainable, total = get_trainable_params_info(model)
print(f"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)")
print(f"   â†’ FC ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥ (ë‚˜ë¨¸ì§€ëŠ” frozen)")

# FC ë ˆì´ì–´ë§Œ í•™ìŠµí•˜ëŠ” optimizer
# í•™ìŠµë¥  1e-3: FC ë ˆì´ì–´ëŠ” ëœë¤ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë¯€ë¡œ í° í•™ìŠµë¥  ì‚¬ìš©
optimizer_step1 = optim.Adam(model.fc.parameters(), lr=1e-3)
scheduler_step1 = optim.lr_scheduler.StepLR(optimizer_step1, step_size=7, gamma=0.1)

print(f"âš™ï¸  Optimizer: Adam (lr=1e-3)")
print(f"   â†’ FC ë ˆì´ì–´ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ìƒëŒ€ì ìœ¼ë¡œ í° í•™ìŠµë¥  ì‚¬ìš©")
print(f"ğŸ“… Max Epochs: 20 (EarlyStopping patience=5)")
print("-" * 70)

# Step 1 í•™ìŠµ (patience=5ë¡œ ì¡°ê¸° ì¢…ë£Œ ê°€ëŠ¥)
trained_model, step1_loss, step1_acc = train_model(
    model, train_dataloader, val_dataloader, criterion, 
    optimizer_step1, scheduler_step1, num_epochs=20, patience=5
)

print(f"\nâœ“ Step 1 ì™„ë£Œ - Val Loss: {step1_loss:.4f}, Val Acc: {step1_acc:.4f}")

# ====================================================================
# Step 2: Layer4 ì–¸í”„ë¦¬ì¦ˆ + Fine-tuning (ì„±ëŠ¥ í–¥ìƒ ì‹œì—ë§Œ ì§„í–‰)
# ====================================================================
# Step 1ì—ì„œ FC ë ˆì´ì–´ê°€ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ì œ ìƒìœ„ ë ˆì´ì–´(layer4)ë¥¼
# ì–¸í”„ë¦¬ì¦ˆí•˜ì—¬ ë„ë©”ì¸ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.
# 
# ResNet18 êµ¬ì¡°: conv1 â†’ layer1 â†’ layer2 â†’ layer3 â†’ layer4 â†’ fc
#                (low-level)                          (high-level)
# 
# Layer4ëŠ” ê³ ìˆ˜ì¤€ íŠ¹ì§•(high-level features)ì„ ì¶”ì¶œí•˜ë¯€ë¡œ,
# ì£¼ì‹ ì°¨íŠ¸ ë„ë©”ì¸ì— ë§ê²Œ ì¡°ì •í•˜ë©´ ì„±ëŠ¥ í–¥ìƒì´ ê¸°ëŒ€ë©ë‹ˆë‹¤.
# 
# ì°¨ë³„ì  í•™ìŠµë¥  ì‚¬ìš©:
# - layer4: 1e-5 (ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ í¬ê²Œ ë³€ê²½í•˜ì§€ ì•ŠìŒ)
# - fc: 1e-4 (ê³„ì† í•™ìŠµ, í•˜ì§€ë§Œ Step 1ë³´ë‹¤ ì‘ê²Œ)
# ====================================================================
print("\n" + "=" * 70)
print("ã€Step 2ã€‘ Layer4 ì–¸í”„ë¦¬ì¦ˆ + Fine-tuning")
print("-" * 70)

# Layer4 ì–¸í”„ë¦¬ì¦ˆ
unfreeze_layer(trained_model, 'layer4')

trainable, total = get_trainable_params_info(trained_model)
print(f"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)")
print(f"   â†’ layer4 + fc í•™ìŠµ ê°€ëŠ¥")

# ì°¨ë³„ì  í•™ìŠµë¥  ì ìš© (Discriminative Learning Rate)
# - layer4: ì‘ì€ learning rate (1e-5) - ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë³´ì¡´
# - fc: ì¤‘ê°„ learning rate (1e-4) - ê³„ì† í•™ìŠµ
optimizer_step2 = optim.Adam([
    {'params': trained_model.layer4.parameters(), 'lr': 1e-5},
    {'params': trained_model.fc.parameters(), 'lr': 1e-4}
])
scheduler_step2 = optim.lr_scheduler.StepLR(optimizer_step2, step_size=5, gamma=0.5)

print(f"âš™ï¸  Optimizer: Adam (Discriminative LR)")
print(f"   - layer4: lr=1e-5 (ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¯¸ì„¸ ì¡°ì •)")
print(f"   - fc: lr=1e-4 (Step 1ì˜ 1/10, ì•ˆì •ì  í•™ìŠµ)")
print(f"ğŸ“… Max Epochs: 15 (EarlyStopping patience=5)")
print(f"ğŸ“‰ Scheduler: StepLR (step=5, gamma=0.5)")
print("-" * 70)

# Step 2 í•™ìŠµ
trained_model, step2_loss, step2_acc = train_model(
    trained_model, train_dataloader, val_dataloader, criterion,
    optimizer_step2, scheduler_step2, num_epochs=15, patience=5
)

print(f"\nâœ“ Step 2 ì™„ë£Œ - Val Loss: {step2_loss:.4f}, Val Acc: {step2_acc:.4f}")

# ====================================================================
# ì„±ëŠ¥ í–¥ìƒ ì²´í¬ - Step 3 ì§„í–‰ ì—¬ë¶€ ê²°ì •
# ====================================================================
# Step 2ì—ì„œ layer4ë¥¼ ì–¸í”„ë¦¬ì¦ˆí•œ ê²°ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
# ê°œì„ ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€ ì–¸í”„ë¦¬ì¦ˆëŠ” ê³¼ì í•©ë§Œ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
# 
# ê°œì„  ê¸°ì¤€:
# - improvement_threshold = 0.5% (ì¡°ì • ê°€ëŠ¥)
# - ì´ ê¸°ì¤€ì€ ë°ì´í„°ì…‹ í¬ê¸°ì™€ ë³µì¡ë„ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
# - ì‘ì€ ë°ì´í„°ì…‹: ë” ë³´ìˆ˜ì  (ì˜ˆ: 1%)
# - í° ë°ì´í„°ì…‹: ë” ê³µê²©ì  (ì˜ˆ: 0.1%)
# ====================================================================
improvement_threshold = 0.005  # 0.5% ì´ìƒ ê°œì„  í•„ìš”
loss_improvement = (step1_loss - step2_loss) / step1_loss

if loss_improvement < improvement_threshold:
    print(f"\nâš ï¸  Step 2 ì„±ëŠ¥ í–¥ìƒ ë¯¸ë¯¸: {loss_improvement*100:.2f}% < {improvement_threshold*100:.1f}%")
    print(f"   â†’ Step 3 (layer3 ì–¸í”„ë¦¬ì¦ˆ)ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    print(f"   â†’ ì¶”ê°€ ì–¸í”„ë¦¬ì¦ˆëŠ” ê³¼ì í•©ë§Œ ìœ ë°œí•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
    skip_step3 = True
else:
    print(f"\nâœ“ Step 2 ì„±ëŠ¥ í–¥ìƒ í™•ì¸: Loss {loss_improvement*100:.2f}% ê°œì„ ")
    print(f"   â†’ Step 3 ì§„í–‰í•©ë‹ˆë‹¤.")
    skip_step3 = False

# ====================================================================
# Step 3: Layer3ê¹Œì§€ ì–¸í”„ë¦¬ì¦ˆ + Deep Fine-tuning (ì¡°ê±´ë¶€ ì‹¤í–‰)
# ====================================================================
# ì¤‘ê°„ ë ˆë²¨ íŠ¹ì§•(layer3)ê¹Œì§€ ë„ë©”ì¸ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.
# 
# ì£¼ì˜ì‚¬í•­:
# 1. Layer3ëŠ” ì¤‘ê°„ ë ˆë²¨ íŠ¹ì§•ì„ ì¶”ì¶œ (shapes, patterns)
# 2. ë„ˆë¬´ ë§ì´ í•™ìŠµí•˜ë©´ ê³¼ì í•© ìœ„í—˜ â†’ ì§§ì€ epoch ì‚¬ìš©
# 3. ë§¤ìš° ì‘ì€ í•™ìŠµë¥  ì‚¬ìš© (5e-6) â†’ ì‚¬ì „í•™ìŠµ ì§€ì‹ ìµœëŒ€í•œ ë³´ì¡´
# 
# í•™ìŠµë¥  ê³„ì¸µ êµ¬ì¡°:
# - layer3: 5e-6 (ê°€ì¥ ì‘ìŒ, ìµœì†Œ ë³€ê²½)
# - layer4: 1e-5 (ì‘ìŒ, ë¯¸ì„¸ ì¡°ì •)
# - fc: 1e-4 (ìƒëŒ€ì ìœ¼ë¡œ í¼, íƒœìŠ¤í¬ íŠ¹í™”)
# 
# í•˜ìœ„ ë ˆì´ì–´(layer1, layer2)ëŠ” frozen ìœ ì§€:
# - ì¼ë°˜ì ì¸ ì €ìˆ˜ì¤€ íŠ¹ì§•(edges, colors)ì€ ëŒ€ë¶€ë¶„ ìœ ìš©
# - ê³¼ì í•© ë°©ì§€ ë° í•™ìŠµ ì‹œê°„ ë‹¨ì¶•
# ====================================================================

if not skip_step3:
    print("\n" + "=" * 70)
    print("ã€Step 3ã€‘ Layer3ê¹Œì§€ ì–¸í”„ë¦¬ì¦ˆ + Deep Fine-tuning")
    print("-" * 70)

    # Layer3 ì–¸í”„ë¦¬ì¦ˆ
    unfreeze_layer(trained_model, 'layer3')

    trainable, total = get_trainable_params_info(trained_model)
    print(f"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)")
    print(f"   â†’ layer3 + layer4 + fc í•™ìŠµ ê°€ëŠ¥")
    print(f"   â†’ layer1, layer2ëŠ” frozen ìœ ì§€ (ì €ìˆ˜ì¤€ íŠ¹ì§• ë³´ì¡´)")

    # ì°¨ë³„ì  í•™ìŠµë¥  ì ìš© (3ë‹¨ê³„ ê³„ì¸µ êµ¬ì¡°)
    # - layer3: ë§¤ìš° ì‘ì€ learning rate (5e-6) - ìµœì†Œ ë³€ê²½
    # - layer4: ì‘ì€ learning rate (1e-5) - ë¯¸ì„¸ ì¡°ì •
    # - fc: ì¤‘ê°„ learning rate (1e-4) - íƒœìŠ¤í¬ íŠ¹í™”
    optimizer_step3 = optim.Adam([
        {'params': trained_model.layer3.parameters(), 'lr': 5e-6},
        {'params': trained_model.layer4.parameters(), 'lr': 1e-5},
        {'params': trained_model.fc.parameters(), 'lr': 1e-4}
    ])
    scheduler_step3 = optim.lr_scheduler.StepLR(optimizer_step3, step_size=3, gamma=0.5)

    print(f"âš™ï¸  Optimizer: Adam (3-tier Discriminative LR)")
    print(f"   - layer3: lr=5e-6 (ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ìµœëŒ€ ë³´ì¡´)")
    print(f"   - layer4: lr=1e-5 (ë¯¸ì„¸ ì¡°ì • ì§€ì†)")
    print(f"   - fc: lr=1e-4 (íƒœìŠ¤í¬ íŠ¹í™” í•™ìŠµ)")
    print(f"ğŸ“… Max Epochs: 10 (EarlyStopping patience=5)")
    print(f"ğŸ“‰ Scheduler: StepLR (step=3, gamma=0.5)")
    print(f"âš ï¸  ì£¼ì˜: ë„ˆë¬´ ë§ì´ í•™ìŠµí•˜ë©´ ê³¼ì í•© ê°€ëŠ¥ì„± ì¦ê°€")
    print("-" * 70)

    # Step 3 í•™ìŠµ
    trained_model, step3_loss, step3_acc = train_model(
        trained_model, train_dataloader, val_dataloader, criterion,
        optimizer_step3, scheduler_step3, num_epochs=10, patience=5
    )

    print(f"\nâœ“ Step 3 ì™„ë£Œ - Val Loss: {step3_loss:.4f}, Val Acc: {step3_acc:.4f}")
    
    # ì„±ëŠ¥ ì²´í¬
    step3_improvement = (step2_loss - step3_loss) / step2_loss
    if step3_improvement < improvement_threshold:
        print(f"\nâš ï¸  Step 3 ì„±ëŠ¥ í–¥ìƒ ë¯¸ë¯¸: {step3_improvement*100:.2f}%")
        print(f"   â†’ Step 2 ëª¨ë¸ì´ ë” ì¢‹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâœ“ Step 3 ì„±ëŠ¥ í–¥ìƒ í™•ì¸: Loss {step3_improvement*100:.2f}% ê°œì„ ")
    
    final_loss = step3_loss
    final_acc = step3_acc
else:
    print("\n" + "=" * 70)
    print("â­ï¸  Step 3 ê±´ë„ˆëœ€ (Step 2 ì„±ëŠ¥ í–¥ìƒ ë¯¸ë¯¸)")
    print("=" * 70)
    final_loss = step2_loss
    final_acc = step2_acc

print("\n" + "=" * 70)
print("âœ… ë‹¨ê³„ì  íŒŒì¸íŠœë‹ ì™„ë£Œ!")
print("=" * 70)
print("ğŸ“Š ìµœì¢… í•™ìŠµ ê²°ê³¼:")
print(f"   - ìµœì¢… Val Loss: {final_loss:.4f}")
print(f"   - ìµœì¢… Val Acc: {final_acc:.4f}")
trainable, total = get_trainable_params_info(trained_model)
print(f"   - í•™ìŠµëœ íŒŒë¼ë¯¸í„°: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)")
print(f"   - Frozen íŒŒë¼ë¯¸í„°: {total - trainable:,} ({(total-trainable)/total*100:.2f}%)")
print("=" * 70)

# ====================================================================
# ìµœì  ì„ê³„ê°’ íƒìƒ‰ (ê²€ì¦ ì„¸íŠ¸ ì‚¬ìš©)
# ====================================================================
# í›ˆë ¨ì´ ì™„ë£Œëœ í›„, ê²€ì¦ ì„¸íŠ¸ì—ì„œ F1-scoreë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì„ê³„ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.
# ì´ ì„ê³„ê°’ì€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
# ====================================================================
optimal_threshold, optimal_val_f1 = find_optimal_threshold(trained_model, val_dataloader, device)

# ====================================================================
# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ (ìµœì  ì„ê³„ê°’ ì‚¬ìš©)
# ====================================================================
# í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ í•´ê²°ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
# F1-ScoreëŠ” ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì˜ ì¡°í™”í‰ê· ìœ¼ë¡œ,
# ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ Accuracyë³´ë‹¤ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì§€í‘œì…ë‹ˆë‹¤.
# 
# ì—¬ê¸°ì„œëŠ” ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì°¾ì€ ìµœì  ì„ê³„ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ====================================================================
print("\n=== Evaluating on Test Set ===")
trained_model.eval()
all_probs = []  # í™•ë¥  ì €ì¥ (ì„ê³„ê°’ ì ìš©ì„ ìœ„í•´)
all_labels = []

with torch.no_grad():
    for inputs, labels in test_dataloader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        
        outputs = trained_model(inputs).squeeze()
        probs = torch.sigmoid(outputs)  # logitsë¥¼ í™•ë¥ ë¡œ ë³€í™˜
        
        all_probs.extend(probs.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

all_probs = np.array(all_probs)
all_labels = np.array(all_labels)

# ====================================================================
# í‰ê°€ ì§€í‘œ ê³„ì‚° - ê¸°ë³¸ ì„ê³„ê°’ 0.5 vs ìµœì  ì„ê³„ê°’
# ====================================================================
# 1. ê¸°ë³¸ ì„ê³„ê°’ 0.5 ì‚¬ìš©
default_preds = (all_probs > 0.5).astype(int)
default_accuracy = accuracy_score(all_labels, default_preds)
default_f1 = f1_score(all_labels, default_preds)

# 2. ìµœì  ì„ê³„ê°’ ì‚¬ìš© (ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì°¾ì€ ê°’)
optimal_preds = (all_probs > optimal_threshold).astype(int)
optimal_accuracy = accuracy_score(all_labels, optimal_preds)
optimal_f1 = f1_score(all_labels, optimal_preds)

print(f"\n" + "=" * 70)
print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ + ì„ê³„ê°’ íŠœë‹)")
print("=" * 70)

print(f"\nã€ê¸°ë³¸ ì„ê³„ê°’ 0.5 ì‚¬ìš©ã€‘")
print(f"   - Test Accuracy: {default_accuracy:.4f} ({default_accuracy*100:.2f}%)")
print(f"   - Test F1-Score: {default_f1:.4f}")

print(f"\nã€ìµœì  ì„ê³„ê°’ {optimal_threshold:.4f} ì‚¬ìš©ã€‘ âœ¨")
print(f"   - Test Accuracy: {optimal_accuracy:.4f} ({optimal_accuracy*100:.2f}%)")
print(f"   - Test F1-Score: {optimal_f1:.4f}")

print(f"\nğŸ“ˆ ì„±ëŠ¥ ê°œì„ :")
print(f"   - Accuracy: {(optimal_accuracy - default_accuracy)*100:+.2f}%p")
print(f"   - F1-Score: {(optimal_f1 - default_f1)*100:+.2f}%p")

print(f"\nğŸ’¡ ìµœì  ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ë©´ F1-scoreê°€ ê°œì„ ë©ë‹ˆë‹¤!")
print("=" * 70)

# ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©í•  ì§€í‘œ (ìµœì  ì„ê³„ê°’ ê¸°ì¤€)
test_accuracy = optimal_accuracy
test_f1 = optimal_f1

# ====================================================================
# ëª¨ë¸ ë° ìµœì  ì„ê³„ê°’ ì €ì¥
# ====================================================================
# ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ìµœì  ì„ê³„ê°’ë„ ì €ì¥í•˜ì—¬ ì¶”ë¡  ì‹œ ë™ì¼í•œ ì„ê³„ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ====================================================================
# ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
torch.save(trained_model.state_dict(), 'resnet18_stock_chart_improved.pth')

# ìµœì  ì„ê³„ê°’ ì €ì¥ (ì¶”ë¡  ì‹œ ì‚¬ìš©)
threshold_info = {
    'optimal_threshold': optimal_threshold,
    'validation_f1': optimal_val_f1,
    'test_accuracy': test_accuracy,
    'test_f1': test_f1
}
torch.save(threshold_info, 'optimal_threshold.pth')

print("\nâœ… ëª¨ë¸ ë° ì„¤ì • ì €ì¥ ì™„ë£Œ:")
print(f"   - ëª¨ë¸ ê°€ì¤‘ì¹˜: resnet18_stock_chart_improved.pth")
print(f"   - ìµœì  ì„ê³„ê°’: optimal_threshold.pth")
print(f"   - ì €ì¥ëœ ì„ê³„ê°’: {optimal_threshold:.4f}")
print(f"\nğŸ’¡ ì¶”ë¡  ì‹œ ì´ ì„ê³„ê°’ì„ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”:")
print(f"   threshold_info = torch.load('optimal_threshold.pth')")
print(f"   threshold = threshold_info['optimal_threshold']")

# ====================================================================
# [ì°¸ê³ ] ì¶”ë¡  ì‹œ ëª¨ë¸ ë° ì„ê³„ê°’ ë¡œë“œ ì˜ˆì œ ì½”ë“œ
# ====================================================================
"""
# ëª¨ë¸ ë¡œë“œ
model = models.resnet18(pretrained=False)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 1)
model.load_state_dict(torch.load('resnet18_stock_chart_improved.pth'))
model.to(device)
model.eval()

# ìµœì  ì„ê³„ê°’ ë¡œë“œ
threshold_info = torch.load('optimal_threshold.pth')
optimal_threshold = threshold_info['optimal_threshold']

# ì¶”ë¡ 
with torch.no_grad():
    outputs = model(input_tensor).squeeze()
    probs = torch.sigmoid(outputs)
    predictions = (probs > optimal_threshold).int()  # ìµœì  ì„ê³„ê°’ ì‚¬ìš©
    
    # 0: í•˜ë½(Down), 1: ìƒìŠ¹(Up)
    print(f"ì˜ˆì¸¡: {'ìƒìŠ¹' if predictions.item() == 1 else 'í•˜ë½'}")
    print(f"í™•ë¥ : {probs.item():.4f}")
    print(f"ì„ê³„ê°’: {optimal_threshold:.4f}")
"""
print("\n" + "=" * 70)
print("ğŸ‰ ëª¨ë¸ í›ˆë ¨ ë° ì„ê³„ê°’ íŠœë‹ ì™„ë£Œ!")
print("=" * 70)
