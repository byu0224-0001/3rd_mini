"""
ì£¼ì‹ ì°¨íŠ¸ íŒ¨í„´ ê¸°ë°˜ CNN ëª¨ë¸ - Fine-tuning + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë²„ì „
- ResNet50 Transfer Learning + Fine-tuning
- ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° (Learning rate, Batch size, Dropout)
- ê°œì„ ëœ Optimizer ë° Learning rate scheduler
- 5,000ì¥ ë°ì´í„°ì…‹ìœ¼ë¡œ íš¨ìœ¨ì  í•™ìŠµ
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import shutil
import random
from pathlib import Path
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,
                             roc_auc_score, roc_curve, f1_score, precision_recall_curve)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import warnings
warnings.filterwarnings('ignore')

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'ignore')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'ignore')

# GPU/CUDA ì„¤ì •
print("ğŸ”§ GPU/CUDA ì„¤ì • ë° ìµœì í™” ì¤‘...")
print(f"TensorFlow ë²„ì „: {tf.__version__}")

try:
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(f"ğŸ” ê°ì§€ëœ GPU: {len(gpus)}ê°œ")
    
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ì‚¬ìš©: {len(gpus)}ê°œ")
    else:
        print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        tf.config.threading.set_inter_op_parallelism_threads(0)
        tf.config.threading.set_intra_op_parallelism_threads(0)
except Exception as e:
    print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")

# ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)
tf.random.set_seed(42)


class FineTunedStockChartCNN:
    """Fine-tuning + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë¸"""
    
    def __init__(self, data_dir, img_size=(128, 128), batch_size=32):
        """
        Args:
            data_dir: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
            img_size: ì´ë¯¸ì§€ í¬ê¸° (ê°œì„ : 100x100 â†’ 128x128)
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.model = None
        self.base_model = None
        self.history_stage1 = None
        self.history_stage2 = None
        self.class_weights = None
        
    def explore_data(self):
        """ë°ì´í„°ì…‹ íƒìƒ‰ ë° í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        print("="*60)
        print("ğŸ“Š ë°ì´í„°ì…‹ íƒìƒ‰")
        print("="*60)
        
        up_dir = os.path.join(self.data_dir, 'up')
        down_dir = os.path.join(self.data_dir, 'down')
        
        up_files = [f for f in os.listdir(up_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        down_files = [f for f in os.listdir(down_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        
        print(f"âœ… ìƒìŠ¹(Up) ì´ë¯¸ì§€: {len(up_files):,}ê°œ")
        print(f"âŒ í•˜ë½(Down) ì´ë¯¸ì§€: {len(down_files):,}ê°œ")
        print(f"ğŸ“ˆ ì´ ì´ë¯¸ì§€: {len(up_files) + len(down_files):,}ê°œ")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        total = len(up_files) + len(down_files)
        weight_down = total / (2 * len(down_files))
        weight_up = total / (2 * len(up_files))
        self.class_weights = {0: weight_down, 1: weight_up}
        
        print(f"âš–ï¸  í´ë˜ìŠ¤ ë¹„ìœ¨: Up={len(up_files)/total*100:.1f}%, Down={len(down_files)/total*100:.1f}%")
        print(f"ğŸ”§ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: Down={weight_down:.4f}, Up={weight_up:.4f}")
        print("="*60)
        
    def create_data_generators(self, validation_split=0.2):
        """ìµœì í™”ëœ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±"""
        print("\nğŸ”„ ìµœì í™”ëœ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„± ì¤‘...")
        print("ğŸ’¡ ê°œì„ ëœ Augmentation:")
        print("   - ì´ë¯¸ì§€ í¬ê¸°: 128x128 (100x100ì—ì„œ ì¦ê°€)")
        print("   - ì‘ì€ íšŒì „: 0-5ë„")
        print("   - ì•½ê°„ì˜ shift, zoom, brightness ì¡°ì •")
        
        # Training ë°ì´í„° ì¦ê°•
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=validation_split,
            rotation_range=5,           # ì•½ê°„ ì¦ê°€
            width_shift_range=0.08,     # ì•½ê°„ ì¦ê°€
            height_shift_range=0.08,    # ì•½ê°„ ì¦ê°€
            zoom_range=0.05,            # ì•½ê°„ ì¦ê°€
            brightness_range=[0.85, 1.15],  # ë²”ìœ„ ì¦ê°€
            horizontal_flip=False,
            vertical_flip=False,
            fill_mode='nearest'
        )
        
        # Validation ë°ì´í„°
        val_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=validation_split
        )
        
        # Training generator
        self.train_generator = train_datagen.flow_from_directory(
            self.data_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            subset='training',
            shuffle=True,
            color_mode='rgb',
            seed=42
        )
        
        # Validation generator
        self.val_generator = val_datagen.flow_from_directory(
            self.data_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            subset='validation',
            shuffle=False,
            color_mode='rgb',
            seed=42
        )
        
        print(f"âœ… Training ìƒ˜í”Œ: {self.train_generator.samples:,}ê°œ")
        print(f"âœ… Validation ìƒ˜í”Œ: {self.val_generator.samples:,}ê°œ")
        print(f"ğŸ“‹ í´ë˜ìŠ¤ ë§¤í•‘: {self.train_generator.class_indices}")
        
    def build_model_stage1(self, dropout_rate=0.4):
        """1ë‹¨ê³„: Transfer Learning ëª¨ë¸ êµ¬ì¶• (Base frozen)"""
        print("\nğŸ—ï¸  1ë‹¨ê³„: Transfer Learning ëª¨ë¸ êµ¬ì¶• (ResNet50)...")
        print("ğŸ’¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì ìš©")
        
        # ResNet50 base model
        base_model = ResNet50(
            include_top=False,
            weights='imagenet',
            input_shape=(self.img_size[0], self.img_size[1], 3),
            pooling='avg'
        )
        
        base_model.trainable = False
        
        print(f"ğŸ“Š Base Model: {len(base_model.layers)}ê°œ ë ˆì´ì–´ (Frozen)")
        
        # Custom top layers (ìµœì í™”ëœ êµ¬ì¡°)
        inputs = keras.Input(shape=(self.img_size[0], self.img_size[1], 3))
        x = base_model(inputs, training=False)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(dropout_rate)(x)
        x = layers.Dense(512, activation='relu', 
                        kernel_regularizer=keras.regularizers.l2(0.001))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(dropout_rate + 0.1)(x)
        x = layers.Dense(256, activation='relu',
                        kernel_regularizer=keras.regularizers.l2(0.001))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(dropout_rate + 0.1)(x)
        outputs = layers.Dense(1, activation='sigmoid')(x)
        
        model = keras.Model(inputs, outputs)
        
        # ìµœì í™”ëœ Optimizer ì„¤ì •
        optimizer = keras.optimizers.Adam(
            learning_rate=0.0001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07
        )
        
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc'),
                keras.metrics.AUC(name='prc', curve='PR')
            ]
        )
        
        self.model = model
        self.base_model = base_model
        
        print("âœ… 1ë‹¨ê³„ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        print(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}ê°œ")
        print(f"   - Dropout: {dropout_rate}")
        print(f"   - L2 Regularization: 0.001")
        
        return model
        
    def build_model_stage2(self, unfreeze_layers=50, learning_rate=0.00001):
        """2ë‹¨ê³„: Fine-tuning ëª¨ë¸ ì„¤ì •"""
        print(f"\nğŸ”“ 2ë‹¨ê³„: Fine-tuning ì¤€ë¹„...")
        print(f"ğŸ’¡ ë§ˆì§€ë§‰ {unfreeze_layers}ê°œ ë ˆì´ì–´ Unfreeze")
        print(f"ğŸ’¡ Learning rate: {learning_rate} (10ë°° ê°ì†Œ)")
        
        # Base model unfreeze
        self.base_model.trainable = True
        frozen_layers = len(self.base_model.layers) - unfreeze_layers
        
        for i, layer in enumerate(self.base_model.layers):
            if i < frozen_layers:
                layer.trainable = False
            else:
                layer.trainable = True
        
        # Fine-tuningìš© ë‚®ì€ learning rateë¡œ ì¬ì»´íŒŒì¼
        optimizer = keras.optimizers.Adam(
            learning_rate=learning_rate,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07
        )
        
        self.model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc'),
                keras.metrics.AUC(name='prc', curve='PR')
            ]
        )
        
        trainable_count = sum([tf.size(w).numpy() for w in self.model.trainable_weights])
        print(f"âœ… Fine-tuning ì¤€ë¹„ ì™„ë£Œ")
        print(f"ğŸ“Š Trainable íŒŒë¼ë¯¸í„°: {trainable_count:,}ê°œ")
        
    def train_stage(self, stage_name, epochs, save_path, patience=15):
        """ë‹¨ê³„ë³„ í•™ìŠµ"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {stage_name} í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ìµœì í™”ëœ Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_auc',
                patience=patience,
                restore_best_weights=True,
                mode='max',
                verbose=1
            ),
            ModelCheckpoint(
                save_path,
                monitor='val_auc',
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=1e-8,
                verbose=1
            )
        ]
        
        print(f"â° ì‹œì‘ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            history = self.model.fit(
                self.train_generator,
                epochs=epochs,
                validation_data=self.val_generator,
                callbacks=callbacks,
                class_weight=self.class_weights,
                verbose=1
            )
            
            print(f"\nâœ… {stage_name} ì™„ë£Œ!")
            print(f"â° ì™„ë£Œ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            return history
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ {stage_name} ì¤‘ë‹¨ë¨")
            return None
            
    def evaluate_comprehensive(self):
        """í¬ê´„ì ì¸ ëª¨ë¸ í‰ê°€"""
        print("\n"+"="*60)
        print("ğŸ“Š ìµœì¢… ëª¨ë¸ í‰ê°€")
        print("="*60)
        
        # ì˜ˆì¸¡
        val_steps = len(self.val_generator)
        y_pred_proba = self.model.predict(self.val_generator, steps=val_steps, verbose=1)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        y_true = self.val_generator.classes
        
        # í‰ê°€ ì§€í‘œ
        accuracy = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        print(f"\nâœ… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"âœ… F1-Score: {f1:.4f}")
        print(f"âœ… ROC-AUC: {roc_auc:.4f}")
        
        print("\nğŸ“‹ Classification Report:")
        print(classification_report(y_true, y_pred, target_names=['Down (0)', 'Up (1)']))
        
        # Confusion Matrix
        cm = confusion_matrix(y_true, y_pred)
        
        # íˆ¬ì ê´€ì  í‰ê°€
        print("\nğŸ’° íˆ¬ì ê´€ì  í‰ê°€:")
        tn, fp, fn, tp = cm.ravel()
        hit_ratio_up = tp / (tp + fp) if (tp + fp) > 0 else 0
        hit_ratio_down = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        print(f"   ğŸ“ˆ ìƒìŠ¹ ì˜ˆì¸¡ ì ì¤‘ë¥ : {hit_ratio_up:.2%}")
        print(f"   ğŸ“‰ í•˜ë½ ì˜ˆì¸¡ ì ì¤‘ë¥ : {hit_ratio_down:.2%}")
        print(f"   ğŸ’¡ ìƒìŠ¹ ì˜ˆì¸¡: {tp+fp}ê±´ ì¤‘ {tp}ê±´ ì ì¤‘")
        print(f"   ğŸ’¡ í•˜ë½ ì˜ˆì¸¡: {tn+fn}ê±´ ì¤‘ {tn}ê±´ ì ì¤‘")
        
        return accuracy, f1, roc_auc, cm, y_true, y_pred_proba.flatten()
        
    def plot_results(self, y_true, y_pred_proba):
        """ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        os.makedirs('results', exist_ok=True)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Stage 1 Accuracy
        if self.history_stage1:
            axes[0, 0].plot(self.history_stage1.history['accuracy'], label='Train', linewidth=2)
            axes[0, 0].plot(self.history_stage1.history['val_accuracy'], label='Val', linewidth=2)
            axes[0, 0].set_title('Stage 1: Transfer Learning Accuracy', fontweight='bold')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].legend()
            axes[0, 0].grid(alpha=0.3)
        
        # 2. Stage 2 Accuracy
        if self.history_stage2:
            axes[0, 1].plot(self.history_stage2.history['accuracy'], label='Train', linewidth=2)
            axes[0, 1].plot(self.history_stage2.history['val_accuracy'], label='Val', linewidth=2)
            axes[0, 1].set_title('Stage 2: Fine-tuning Accuracy', fontweight='bold')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Accuracy')
            axes[0, 1].legend()
            axes[0, 1].grid(alpha=0.3)
        
        # 3. Combined AUC
        if self.history_stage1 and self.history_stage2:
            epochs_s1 = len(self.history_stage1.history['auc'])
            epochs_s2 = len(self.history_stage2.history['auc'])
            
            axes[0, 2].plot(range(1, epochs_s1+1), self.history_stage1.history['auc'], 
                          label='Stage 1 Train', linewidth=2, color='blue')
            axes[0, 2].plot(range(1, epochs_s1+1), self.history_stage1.history['val_auc'], 
                          label='Stage 1 Val', linewidth=2, color='lightblue')
            axes[0, 2].plot(range(epochs_s1+1, epochs_s1+epochs_s2+1), self.history_stage2.history['auc'], 
                          label='Stage 2 Train', linewidth=2, color='red')
            axes[0, 2].plot(range(epochs_s1+1, epochs_s1+epochs_s2+1), self.history_stage2.history['val_auc'], 
                          label='Stage 2 Val', linewidth=2, color='salmon')
            axes[0, 2].axvline(x=epochs_s1, color='green', linestyle='--', linewidth=2, label='Fine-tuning ì‹œì‘')
            axes[0, 2].set_title('ROC-AUC (2-Stage)', fontweight='bold')
            axes[0, 2].set_xlabel('Epoch')
            axes[0, 2].set_ylabel('AUC')
            axes[0, 2].legend()
            axes[0, 2].grid(alpha=0.3)
        
        # 4. ROC Curve
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        axes[1, 0].plot(fpr, tpr, linewidth=3, label=f'ROC (AUC = {roc_auc:.4f})', color='blue')
        axes[1, 0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
        axes[1, 0].set_title('ROC Curve', fontweight='bold')
        axes[1, 0].set_xlabel('False Positive Rate')
        axes[1, 0].set_ylabel('True Positive Rate')
        axes[1, 0].legend()
        axes[1, 0].grid(alpha=0.3)
        axes[1, 0].fill_between(fpr, tpr, alpha=0.2)
        
        # 5. Precision-Recall Curve
        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
        
        axes[1, 1].plot(recall, precision, linewidth=3, label='PR Curve', color='green')
        axes[1, 1].set_title('Precision-Recall Curve', fontweight='bold')
        axes[1, 1].set_xlabel('Recall')
        axes[1, 1].set_ylabel('Precision')
        axes[1, 1].legend()
        axes[1, 1].grid(alpha=0.3)
        axes[1, 1].fill_between(recall, precision, alpha=0.2)
        
        # 6. Prediction Distribution
        axes[1, 2].hist(y_pred_proba[y_true == 0], bins=50, alpha=0.6, label='Down (ì‹¤ì œ)', color='red', edgecolor='darkred')
        axes[1, 2].hist(y_pred_proba[y_true == 1], bins=50, alpha=0.6, label='Up (ì‹¤ì œ)', color='blue', edgecolor='darkblue')
        axes[1, 2].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')
        axes[1, 2].set_title('Prediction Distribution', fontweight='bold')
        axes[1, 2].set_xlabel('Predicted Probability')
        axes[1, 2].set_ylabel('Frequency')
        axes[1, 2].legend()
        axes[1, 2].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/finetuned_results.png', dpi=300, bbox_inches='tight')
        print("âœ… ì €ì¥: results/finetuned_results.png")
        plt.close()
        
        # Confusion Matrix
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', cbar=True,
                   xticklabels=['Down (0)', 'Up (1)'],
                   yticklabels=['Down (0)', 'Up (1)'],
                   annot_kws={'size': 16})
        plt.title('Fine-tuned Model - Confusion Matrix', fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        
        accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()
        plt.text(1, -0.3, f'Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)',
                ha='center', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('results/finetuned_confusion_matrix.png', dpi=300, bbox_inches='tight')
        print("âœ… ì €ì¥: results/finetuned_confusion_matrix.png")
        plt.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n"+"="*60)
    print("ğŸš€ Fine-tuning + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë¸")
    print("ğŸ’¡ 2ë‹¨ê³„ í•™ìŠµìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±")
    print("="*60)
    
    # ì„¤ì •
    gpus = tf.config.experimental.list_physical_devices('GPU')
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    hyperparams = {
        'img_size': (128, 128),      # ì´ë¯¸ì§€ í¬ê¸° ì¦ê°€
        'batch_size': 64 if not gpus else 128,  # GPU ìˆìœ¼ë©´ ë” í° ë°°ì¹˜
        'dropout_rate': 0.4,         # Dropout ë¹„ìœ¨
        'stage1_epochs': 50,         # 1ë‹¨ê³„ ì—í¬í¬
        'stage2_epochs': 50,         # 2ë‹¨ê³„ ì—í¬í¬
        'stage1_lr': 0.0001,         # 1ë‹¨ê³„ learning rate
        'stage2_lr': 0.00001,        # 2ë‹¨ê³„ learning rate (10ë°° ë‚®ê²Œ)
        'unfreeze_layers': 60,       # Unfreezeí•  ë ˆì´ì–´ ìˆ˜
        'patience': 15               # Early stopping patience
    }
    
    print(f"\nğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •:")
    for key, value in hyperparams.items():
        print(f"   - {key}: {value}")
    print("="*60)
    
    # ë°ì´í„°ì…‹ í™•ì¸
    data_dir = 'dataset-subset-5k'
    if not os.path.exists(data_dir):
        print(f"\nâŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        print("ğŸ’¡ ë¨¼ì € stock_chart_cnn_improved.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì„œë¸Œì…‹ì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = FineTunedStockChartCNN(
        data_dir=data_dir,
        img_size=hyperparams['img_size'],
        batch_size=hyperparams['batch_size']
    )
    
    # ë°ì´í„° íƒìƒ‰
    model.explore_data()
    
    # ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
    model.create_data_generators(validation_split=0.2)
    
    # ===== STAGE 1: Transfer Learning =====
    print("\n"+"#"*60)
    print("ğŸ“Œ STAGE 1: Transfer Learning (Base Frozen)")
    print("#"*60)
    
    model.build_model_stage1(dropout_rate=hyperparams['dropout_rate'])
    
    model.history_stage1 = model.train_stage(
        stage_name="Stage 1 - Transfer Learning",
        epochs=hyperparams['stage1_epochs'],
        save_path='models/finetuned/stage1_best.keras',
        patience=hyperparams['patience']
    )
    
    # ===== STAGE 2: Fine-tuning =====
    print("\n"+"#"*60)
    print("ğŸ“Œ STAGE 2: Fine-tuning (Partial Unfreeze)")
    print("#"*60)
    
    model.build_model_stage2(
        unfreeze_layers=hyperparams['unfreeze_layers'],
        learning_rate=hyperparams['stage2_lr']
    )
    
    model.history_stage2 = model.train_stage(
        stage_name="Stage 2 - Fine-tuning",
        epochs=hyperparams['stage2_epochs'],
        save_path='models/finetuned/stage2_best.keras',
        patience=hyperparams['patience']
    )
    
    # ===== ìµœì¢… í‰ê°€ =====
    accuracy, f1, roc_auc, cm, y_true, y_pred_proba = model.evaluate_comprehensive()
    
    # ê²°ê³¼ ì‹œê°í™”
    model.plot_results(y_true, y_pred_proba)
    
    # ìµœì¢… ìš”ì•½
    print("\n"+"="*60)
    print("ğŸ‰ Fine-tuning + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“Š ìµœì¢… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"ğŸ“Š F1-Score: {f1:.4f}")
    print(f"ğŸ“Š ROC-AUC: {roc_auc:.4f}")
    print("="*60)
    
    # ì„±ëŠ¥ ë¹„êµ
    print("\n"+"="*60)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print("="*60)
    print("ê¸°ì¡´ ë‹¨ìˆœ CNN:")
    print("   - Accuracy: 54.96%")
    print("\nì´ì „ Transfer Learning (1ë‹¨ê³„ë§Œ):")
    print("   - Accuracy: 53.30%")
    print("\ní˜„ì¬ Fine-tuned + ìµœì í™”:")
    print(f"   - Accuracy: {accuracy*100:.2f}%")
    print(f"   - F1-Score: {f1:.4f}")
    print(f"   - ROC-AUC: {roc_auc:.4f}")
    print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì ìš©")
    print(f"   - 2ë‹¨ê³„ Fine-tuning ì™„ë£Œ")
    
    improvement = ((accuracy - 0.5496) / 0.5496) * 100
    print(f"\nğŸ“ˆ ê¸°ì¡´ ëŒ€ë¹„ ì„±ëŠ¥ ë³€í™”: {improvement:+.2f}%")
    print("="*60)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìš”ì•½
    print("\n"+"="*60)
    print("âš™ï¸  ìµœì¢… í•˜ì´í¼íŒŒë¼ë¯¸í„°")
    print("="*60)
    for key, value in hyperparams.items():
        print(f"   {key}: {value}")
    print("="*60)


if __name__ == '__main__':
    main()

