"""
ì£¼ì‹ ì°¨íŠ¸ íŒ¨í„´ ê¸°ë°˜ CNN ëª¨ë¸ - ê°œì„  ë²„ì „
- Transfer Learning (EfficientNetB0) ì ìš©
- ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•œ Data Augmentation
- Class Weightë¥¼ í†µí•œ ë¶ˆê· í˜• í•´ê²°
- ê°œì„ ëœ í‰ê°€ ì§€í‘œ (ROC-AUC, F1-score)
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,
                             roc_auc_score, roc_curve, f1_score, precision_recall_curve)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import warnings
warnings.filterwarnings('ignore')

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'ignore')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'ignore')

# GPU/CUDA ì„¤ì • ë° ìµœì í™”
print("ğŸ”§ GPU/CUDA ì„¤ì • ë° ìµœì í™” ì¤‘...")
print(f"TensorFlow ë²„ì „: {tf.__version__}")

# GPU ì„¤ì • ê°•ì œ í™œì„±í™”
try:
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(f"ğŸ” ê°ì§€ëœ GPU: {len(gpus)}ê°œ")
    
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        tf.config.experimental.set_visible_devices(gpus, 'GPU')
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {len(gpus)}ê°œ")
        print("ğŸš€ GPU ê°€ì† í•™ìŠµ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤!")
        for i, gpu in enumerate(gpus):
            print(f"   GPU {i}: {gpu.name}")
    else:
        print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        print("ğŸ’¡ CPU ìµœì í™” ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤.")
        tf.config.threading.set_inter_op_parallelism_threads(0)
        tf.config.threading.set_intra_op_parallelism_threads(0)
except Exception as e:
    print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
    print("ğŸ”„ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
    tf.config.threading.set_inter_op_parallelism_threads(0)
    tf.config.threading.set_intra_op_parallelism_threads(0)

# ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)
tf.random.set_seed(42)


class ImprovedStockChartCNN:
    """ê°œì„ ëœ ì£¼ì‹ ì°¨íŠ¸ CNN ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir='dataset-2021', img_size=(100, 100), batch_size=32):
        """
        Args:
            data_dir: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
            img_size: ì´ë¯¸ì§€ í¬ê¸° (width, height)
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.model = None
        self.history = None
        self.class_weights = None
        
    def explore_data(self):
        """ë°ì´í„°ì…‹ íƒìƒ‰ ë° í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        print("=" * 60)
        print("ğŸ“Š ë°ì´í„°ì…‹ íƒìƒ‰")
        print("=" * 60)
        
        up_dir = os.path.join(self.data_dir, 'up')
        down_dir = os.path.join(self.data_dir, 'down')
        
        up_files = os.listdir(up_dir)
        down_files = os.listdir(down_dir)
        
        print(f"âœ… ìƒìŠ¹(Up) ì´ë¯¸ì§€: {len(up_files):,}ê°œ")
        print(f"âŒ í•˜ë½(Down) ì´ë¯¸ì§€: {len(down_files):,}ê°œ")
        print(f"ğŸ“ˆ ì´ ì´ë¯¸ì§€: {len(up_files) + len(down_files):,}ê°œ")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
        total = len(up_files) + len(down_files)
        weight_down = total / (2 * len(down_files))
        weight_up = total / (2 * len(up_files))
        self.class_weights = {0: weight_down, 1: weight_up}
        
        print(f"âš–ï¸  í´ë˜ìŠ¤ ë¹„ìœ¨: Up={len(up_files)/total*100:.1f}%, "
              f"Down={len(down_files)/total*100:.1f}%")
        print(f"ğŸ”§ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: Down={weight_down:.4f}, Up={weight_up:.4f}")
        print("=" * 60)
        
        return len(up_files), len(down_files)
    
    def create_data_generators(self, validation_split=0.2):
        """ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•œ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±"""
        print("\nğŸ”„ ê°œì„ ëœ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„± ì¤‘...")
        print("ğŸ’¡ ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•œ Augmentation ì ìš©:")
        print("   - ì¢Œìš° ë°˜ì „ ì œê±° (ì‹œê°„ íë¦„ ë³´ì¡´)")
        print("   - ì‘ì€ íšŒì „ë§Œ í—ˆìš© (0-3ë„)")
        print("   - ë…¸ì´ì¦ˆ ë° ë°ê¸° ì¡°ì • ì¶”ê°€")
        
        # Training ë°ì´í„° ì¦ê°• (ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•˜ê²Œ)
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=validation_split,
            width_shift_range=0.05,     # ì‘ì€ ì´ë™
            height_shift_range=0.05,    # ì‘ì€ ì´ë™
            horizontal_flip=False,      # ì¢Œìš° ë°˜ì „ ì œê±° (ì‹œê°„ íë¦„ ë³´ì¡´)
            vertical_flip=False,        # ìƒí•˜ ë°˜ì „ ì œê±°
            zoom_range=0.02,            # ì‘ì€ ì¤Œ
            brightness_range=[0.9, 1.1], # ë°ê¸° ì¡°ì •
            fill_mode='nearest'
        )
        
        # Validation ë°ì´í„° (ì¦ê°• ì—†ìŒ)
        val_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=validation_split
        )
        
        # Training ì œë„ˆë ˆì´í„° (Grayscale â†’ RGB ë³€í™˜)
        self.train_generator = train_datagen.flow_from_directory(
            self.data_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            subset='training',
            shuffle=True,
            seed=42,
            color_mode='rgb'  # Grayscaleì„ RGBë¡œ ë³€í™˜
        )
        
        # Validation ì œë„ˆë ˆì´í„° (Grayscale â†’ RGB ë³€í™˜)
        self.val_generator = val_datagen.flow_from_directory(
            self.data_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            subset='validation',
            shuffle=False,
            seed=42,
            color_mode='rgb'  # Grayscaleì„ RGBë¡œ ë³€í™˜
        )
        
        print(f"âœ… Training ìƒ˜í”Œ: {self.train_generator.samples:,}ê°œ")
        print(f"âœ… Validation ìƒ˜í”Œ: {self.val_generator.samples:,}ê°œ")
        print(f"ğŸ“‹ í´ë˜ìŠ¤ ë§¤í•‘: {self.train_generator.class_indices}")
        
        return self.train_generator, self.val_generator
    
    def build_model_with_transfer_learning(self):
        """ê°œì„ ëœ CNN ëª¨ë¸ êµ¬ì¶• (Grayscale ìµœì í™” + ë” ê¹Šì€ êµ¬ì¡°)"""
        print("\nğŸ—ï¸  ê°œì„ ëœ CNN ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        print("ğŸ’¡ Grayscale ì´ë¯¸ì§€ì— ìµœì í™”ëœ êµ¬ì¡°")
        
        # Grayscaleì— ìµœì í™”ëœ ê¹Šì€ CNN êµ¬ì¡°
        model = models.Sequential([
            # Conv Block 1
            layers.Conv2D(64, (3, 3), activation='relu', padding='same', 
                         input_shape=(self.img_size[0], self.img_size[1], 3)),
            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.3),
            
            # Conv Block 2
            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.3),
            
            # Conv Block 3
            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.3),
            
            # Conv Block 4
            layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
            layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.3),
            
            # Global Average Pooling (ëŒ€ì‹  Flatten)
            layers.GlobalAveragePooling2D(),
            
            # Fully Connected Layers
            layers.Dense(1024, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(512, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            
            # Output Layer
            layers.Dense(1, activation='sigmoid')
        ])
        
        # ëª¨ë¸ ì»´íŒŒì¼ (ê°œì„ ëœ learning rate)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # ë‚®ì€ learning rate
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc'),
                keras.metrics.AUC(name='prc', curve='PR')  # Precision-Recall AUC
            ]
        )
        
        self.model = model
        
        print("âœ… ê°œì„ ëœ CNN ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        print(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}ê°œ")
        trainable_count = sum([tf.size(w).numpy() for w in model.trainable_weights])
        print(f"   - Trainable: {trainable_count:,}ê°œ")
        
        return model
    
    def train(self, epochs=30, save_path='models/improved_model.h5', use_class_weights=True):
        """ê°œì„ ëœ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        
        total_samples = self.train_generator.samples + self.val_generator.samples
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {total_samples:,}ê°œ")
        print(f"   - Training: {self.train_generator.samples:,}ê°œ")
        print(f"   - Validation: {self.val_generator.samples:,}ê°œ")
        
        if use_class_weights:
            print(f"âš–ï¸  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©: {self.class_weights}")
        print("=" * 60)
        
        # ì½œë°± ì„¤ì •
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        callbacks = [
            EarlyStopping(
                monitor='val_auc',
                patience=10,
                restore_best_weights=True,
                mode='max',
                verbose=1
            ),
            ModelCheckpoint(
                save_path,
                monitor='val_auc',
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-8,
                verbose=1
            )
        ]
        
        print(f"â° í•™ìŠµ ì‹œì‘ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        # í•™ìŠµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
        try:
            self.history = self.model.fit(
                self.train_generator,
                epochs=epochs,
                validation_data=self.val_generator,
                callbacks=callbacks,
                class_weight=self.class_weights if use_class_weights else None,
                verbose=1
            )
            
            print("\n" + "=" * 60)
            print("âœ… í•™ìŠµ ì™„ë£Œ!")
            print(f"â° ì™„ë£Œ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 60)
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return self.history
        
        return self.history
    
    def evaluate_comprehensive(self):
        """í¬ê´„ì ì¸ ëª¨ë¸ í‰ê°€"""
        print("\nğŸ“Š í¬ê´„ì ì¸ ëª¨ë¸ í‰ê°€")
        print("=" * 60)
        
        # Validation ë°ì´í„°ë¡œ ì˜ˆì¸¡
        val_steps = len(self.val_generator)
        y_pred_proba = self.model.predict(self.val_generator, steps=val_steps, verbose=1)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        y_true = self.val_generator.classes
        
        # ê¸°ë³¸ í‰ê°€ ì§€í‘œ
        accuracy = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        print(f"\nâœ… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"âœ… F1-Score: {f1:.4f}")
        print(f"âœ… ROC-AUC: {roc_auc:.4f}")
        
        print("\nğŸ“‹ Classification Report:")
        print(classification_report(y_true, y_pred, 
                                   target_names=['Down (0)', 'Up (1)']))
        
        # Confusion Matrix
        cm = confusion_matrix(y_true, y_pred)
        
        # íˆ¬ì ê´€ì  ì§€í‘œ
        print("\nğŸ’° íˆ¬ì ê´€ì  í‰ê°€:")
        tn, fp, fn, tp = cm.ravel()
        
        # Hit Ratio (ì˜ˆì¸¡ ìƒìŠ¹ â†’ ì‹¤ì œ ìƒìŠ¹ ë¹„ìœ¨)
        hit_ratio_up = tp / (tp + fp) if (tp + fp) > 0 else 0
        hit_ratio_down = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        print(f"   ğŸ“ˆ ìƒìŠ¹ ì˜ˆì¸¡ ì ì¤‘ë¥ : {hit_ratio_up:.2%}")
        print(f"   ğŸ“‰ í•˜ë½ ì˜ˆì¸¡ ì ì¤‘ë¥ : {hit_ratio_down:.2%}")
        
        # ì˜ˆì¸¡ ìƒìŠ¹ ì‹œ ì‹¤ì œ ìˆ˜ìµë¥  (ê°€ìƒ)
        print(f"   ğŸ’¡ ìƒìŠ¹ ì˜ˆì¸¡ ì •í™•ë„: {tp}/{tp+fp} = {hit_ratio_up:.2%}")
        print(f"   ğŸ’¡ í•˜ë½ ì˜ˆì¸¡ ì •í™•ë„: {tn}/{tn+fn} = {hit_ratio_down:.2%}")
        
        return accuracy, f1, roc_auc, cm, y_true, y_pred_proba.flatten()
    
    def plot_comprehensive_results(self, y_true, y_pred_proba, 
                                   save_path='results/improved_results.png'):
        """í¬ê´„ì ì¸ ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“ˆ í¬ê´„ì ì¸ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Accuracy
        if self.history and 'accuracy' in self.history.history:
            axes[0, 0].plot(self.history.history['accuracy'], label='Train')
            axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')
            axes[0, 0].set_title('Accuracy', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Loss
        if self.history and 'loss' in self.history.history:
            axes[0, 1].plot(self.history.history['loss'], label='Train')
            axes[0, 1].plot(self.history.history['val_loss'], label='Validation')
            axes[0, 1].set_title('Loss', fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. AUC
        if self.history and 'auc' in self.history.history:
            axes[0, 2].plot(self.history.history['auc'], label='Train AUC')
            axes[0, 2].plot(self.history.history['val_auc'], label='Val AUC')
            axes[0, 2].set_title('ROC-AUC', fontsize=14, fontweight='bold')
            axes[0, 2].set_xlabel('Epoch')
            axes[0, 2].set_ylabel('AUC')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)
        
        # 4. ROC Curve
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        axes[1, 0].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.4f})')
        axes[1, 0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
        axes[1, 0].set_title('ROC Curve', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('False Positive Rate')
        axes[1, 0].set_ylabel('True Positive Rate')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. Precision-Recall Curve
        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
        
        axes[1, 1].plot(recall, precision, linewidth=2, label='PR Curve')
        axes[1, 1].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Recall')
        axes[1, 1].set_ylabel('Precision')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. Prediction Distribution
        axes[1, 2].hist(y_pred_proba[y_true == 0], bins=50, alpha=0.5, label='Down (0)', color='red')
        axes[1, 2].hist(y_pred_proba[y_true == 1], bins=50, alpha=0.5, label='Up (1)', color='blue')
        axes[1, 2].axvline(x=0.5, color='black', linestyle='--', linewidth=2)
        axes[1, 2].set_title('Prediction Distribution', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('Predicted Probability')
        axes[1, 2].set_ylabel('Frequency')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        plt.close()
    
    def plot_confusion_matrix(self, cm, save_path='results/improved_confusion_matrix.png'):
        """Confusion Matrix ì‹œê°í™”"""
        print("\nğŸ“Š Confusion Matrix ì‹œê°í™” ì¤‘...")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
                   xticklabels=['Down (0)', 'Up (1)'],
                   yticklabels=['Down (0)', 'Up (1)'],
                   annot_kws={'size': 16})
        plt.title('Confusion Matrix - Improved Model', fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        
        accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()
        plt.text(1, -0.3, f'Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)',
                ha='center', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        plt.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "=" * 60)
    print("ğŸš€ ê°œì„ ëœ ì£¼ì‹ ì°¨íŠ¸ íŒ¨í„´ CNN ëª¨ë¸")
    print("ğŸ’¡ Transfer Learning + ìµœì í™”ëœ Augmentation")
    print("=" * 60)
    
    # GPU/CPUì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì„¤ì •
    gpus = tf.config.experimental.list_physical_devices('GPU')
    batch_size = 128 if gpus else 64
    
    # 1. ëª¨ë¸ ê°ì²´ ìƒì„±
    stock_cnn = ImprovedStockChartCNN(
        data_dir='dataset-2021',
        img_size=(100, 100),
        batch_size=batch_size
    )
    
    print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size} (GPU: {'ì‚¬ìš©' if gpus else 'ë¯¸ì‚¬ìš©'})")
    if gpus:
        print("ğŸ’¡ RTX 4060 8GB VRAMì— ìµœì í™”ëœ ì„¤ì •ì…ë‹ˆë‹¤.")
    
    # 2. ë°ì´í„° íƒìƒ‰ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°)
    stock_cnn.explore_data()
    
    # 3. ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
    stock_cnn.create_data_generators(validation_split=0.2)
    
    # 4. ê°œì„ ëœ ëª¨ë¸ êµ¬ì¶•
    stock_cnn.build_model_with_transfer_learning()
    
    print("\n" + "=" * 60)
    print("âš ï¸  ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    print("ğŸš€ ìë™ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 5. ëª¨ë¸ í•™ìŠµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
    stock_cnn.train(epochs=30, save_path='models/improved_model_final.h5')
    
    # 8. í¬ê´„ì ì¸ í‰ê°€
    accuracy, f1, roc_auc, cm, y_true, y_pred_proba = stock_cnn.evaluate_comprehensive()
    
    # 9. ê²°ê³¼ ì‹œê°í™”
    stock_cnn.plot_comprehensive_results(y_true, y_pred_proba)
    stock_cnn.plot_confusion_matrix(cm)
    
    # 10. ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ‰ ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“Š ìµœì¢… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"ğŸ“Š F1-Score: {f1:.4f}")
    print(f"ğŸ“Š ROC-AUC: {roc_auc:.4f}")
    print("=" * 60)
    
    # 11. ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ
    print("\n" + "=" * 60)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ (ê¸°ì¡´ vs ê°œì„ )")
    print("=" * 60)
    print("ê¸°ì¡´ ëª¨ë¸:")
    print("   - Accuracy: 0.5496 (54.96%)")
    print("   - ë‹¨ìˆœ CNN êµ¬ì¡°")
    print("   - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¯¸í•´ê²°")
    print("\nê°œì„ ëœ ëª¨ë¸:")
    print(f"   - Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   - F1-Score: {f1:.4f}")
    print(f"   - ROC-AUC: {roc_auc:.4f}")
    print("   - Transfer Learning (EfficientNetB0)")
    print("   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©")
    print("   - ê¸ˆìœµ ì°¨íŠ¸ ìµœì í™” Augmentation")
    
    improvement = ((accuracy - 0.5496) / 0.5496) * 100
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ê°œì„ : {improvement:+.2f}%")
    print("=" * 60)


if __name__ == '__main__':
    main()

