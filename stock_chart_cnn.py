"""

ì£¼ì‹ ì°¨íŠ¸ íŒ¨í„´ ê¸°ë°˜ CNN ëª¨ë¸

- ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ìŒë‚  ìƒìŠ¹/í•˜ë½ ì˜ˆì¸¡

- Dataset: dataset-2021/up, dataset-2021/down

"""



import os

import sys

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from PIL import Image

from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import tensorflow as tf

from tensorflow import keras

from tensorflow.keras import layers, models

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

from tensorflow.keras.preprocessing.image import ImageDataGenerator

import warnings

warnings.filterwarnings('ignore')



# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •

if sys.platform == 'win32':

Â  Â  import codecs

Â  Â  sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'ignore')

Â  Â  sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'ignore')



# GPU/CUDA ì„¤ì • ë° ìµœì í™”

print("ğŸ”§ GPU/CUDA ì„¤ì • ë° ìµœì í™” ì¤‘...")

print(f"TensorFlow ë²„ì „: {tf.__version__}")



# GPU ì„¤ì • ê°•ì œ í™œì„±í™”

try:

Â  Â  # GPU ë””ë°”ì´ìŠ¤ í™•ì¸

Â  Â  gpus = tf.config.experimental.list_physical_devices('GPU')

Â  Â  print(f"ğŸ” ê°ì§€ëœ GPU: {len(gpus)}ê°œ")

Â  Â  

Â  Â  if gpus:

Â  Â  Â  Â  # GPU ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì •

Â  Â  Â  Â  for gpu in gpus:

Â  Â  Â  Â  Â  Â  tf.config.experimental.set_memory_growth(gpu, True)

Â  Â  Â  Â  

Â  Â  Â  Â  # GPU ì‚¬ìš© ì„¤ì •

Â  Â  Â  Â  tf.config.experimental.set_visible_devices(gpus, 'GPU')

Â  Â  Â  Â  print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {len(gpus)}ê°œ")

Â  Â  Â  Â  print("âœ… GPU ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì • ì™„ë£Œ")

Â  Â  Â  Â  print("ğŸš€ GPU ê°€ì† í•™ìŠµ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤!")

Â  Â  Â  Â  

Â  Â  Â  Â  # GPU ì •ë³´ ì¶œë ¥

Â  Â  Â  Â  for i, gpu in enumerate(gpus):

Â  Â  Â  Â  Â  Â  print(f" Â  GPU {i}: {gpu.name}")

Â  Â  else:

Â  Â  Â  Â  print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")

Â  Â  Â  Â  print("ğŸ’¡ CPU ìµœì í™” ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤.")

Â  Â  Â  Â  

Â  Â  Â  Â  # CPU ìµœì í™” ì„¤ì •

Â  Â  Â  Â  tf.config.threading.set_inter_op_parallelism_threads(0)

Â  Â  Â  Â  tf.config.threading.set_intra_op_parallelism_threads(0)

Â  Â  Â  Â  

except Exception as e:

Â  Â  print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")

Â  Â  print("ğŸ”„ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

Â  Â  

Â  Â  # CPU ìµœì í™” ì„¤ì •

Â  Â  tf.config.threading.set_inter_op_parallelism_threads(0)

Â  Â  tf.config.threading.set_intra_op_parallelism_threads(0)



# ëœë¤ ì‹œë“œ ê³ ì •

np.random.seed(42)

tf.random.set_seed(42)



class StockChartCNN:

Â  Â  """ì£¼ì‹ ì°¨íŠ¸ CNN ëª¨ë¸ í´ë˜ìŠ¤"""

Â  Â  

Â  Â  def __init__(self, data_dir='dataset-2021', img_size=(100, 100), batch_size=32):

Â  Â  Â  Â  """

Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  data_dir: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬

Â  Â  Â  Â  Â  Â  img_size: ì´ë¯¸ì§€ í¬ê¸° (width, height)

Â  Â  Â  Â  Â  Â  batch_size: ë°°ì¹˜ í¬ê¸°

Â  Â  Â  Â  """

Â  Â  Â  Â  self.data_dir = data_dir

Â  Â  Â  Â  self.img_size = img_size

Â  Â  Â  Â  self.batch_size = batch_size

Â  Â  Â  Â  self.model = None

Â  Â  Â  Â  self.history = None

Â  Â  Â  Â  

Â  Â  def explore_data(self):

Â  Â  Â  Â  """ë°ì´í„°ì…‹ íƒìƒ‰"""

Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  print("ğŸ“Š ë°ì´í„°ì…‹ íƒìƒ‰")

Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  

Â  Â  Â  Â  up_dir = os.path.join(self.data_dir, 'up')

Â  Â  Â  Â  down_dir = os.path.join(self.data_dir, 'down')

Â  Â  Â  Â  

Â  Â  Â  Â  up_files = os.listdir(up_dir)

Â  Â  Â  Â  down_files = os.listdir(down_dir)

Â  Â  Â  Â  

Â  Â  Â  Â  print(f"âœ… ìƒìŠ¹(Up) ì´ë¯¸ì§€: {len(up_files):,}ê°œ")

Â  Â  Â  Â  print(f"âŒ í•˜ë½(Down) ì´ë¯¸ì§€: {len(down_files):,}ê°œ")

Â  Â  Â  Â  print(f"ğŸ“ˆ ì´ ì´ë¯¸ì§€: {len(up_files) + len(down_files):,}ê°œ")

Â  Â  Â  Â  print(f"âš–ï¸ Â í´ë˜ìŠ¤ ë¹„ìœ¨: Up={len(up_files)/(len(up_files)+len(down_files))*100:.1f}%, "

Â  Â  Â  Â  Â  Â  Â  f"Down={len(down_files)/(len(up_files)+len(down_files))*100:.1f}%")

Â  Â  Â  Â  

Â  Â  Â  Â  # ìƒ˜í”Œ ì´ë¯¸ì§€ í¬ê¸° í™•ì¸

Â  Â  Â  Â  sample_img_path = os.path.join(up_dir, up_files[0])

Â  Â  Â  Â  sample_img = Image.open(sample_img_path)

Â  Â  Â  Â  print(f"ğŸ“ ìƒ˜í”Œ ì´ë¯¸ì§€ í¬ê¸°: {sample_img.size}")

Â  Â  Â  Â  print(f"ğŸ¨ ì´ë¯¸ì§€ ëª¨ë“œ: {sample_img.mode}")

Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  

Â  Â  Â  Â  return len(up_files), len(down_files)

Â  Â  

Â  Â  def create_data_generators(self, validation_split=0.2):

Â  Â  Â  Â  """ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„± (Data Augmentation í¬í•¨)"""

Â  Â  Â  Â  print("\nğŸ”„ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„± ì¤‘...")

Â  Â  Â  Â  

Â  Â  Â  Â  # Training ë°ì´í„° ì¦ê°•

Â  Â  Â  Â  train_datagen = ImageDataGenerator(

Â  Â  Â  Â  Â  Â  rescale=1./255,

Â  Â  Â  Â  Â  Â  validation_split=validation_split,

Â  Â  Â  Â  Â  Â  rotation_range=10,

Â  Â  Â  Â  Â  Â  width_shift_range=0.1,

Â  Â  Â  Â  Â  Â  height_shift_range=0.1,

Â  Â  Â  Â  Â  Â  horizontal_flip=True,

Â  Â  Â  Â  Â  Â  zoom_range=0.1,

Â  Â  Â  Â  Â  Â  fill_mode='nearest'

Â  Â  Â  Â  )

Â  Â  Â  Â  

Â  Â  Â  Â  # Validation ë°ì´í„° (ì¦ê°• ì—†ìŒ)

Â  Â  Â  Â  val_datagen = ImageDataGenerator(

Â  Â  Â  Â  Â  Â  rescale=1./255,

Â  Â  Â  Â  Â  Â  validation_split=validation_split

Â  Â  Â  Â  )

Â  Â  Â  Â  

Â  Â  Â  Â  # Training ì œë„ˆë ˆì´í„°

Â  Â  Â  Â  self.train_generator = train_datagen.flow_from_directory(

Â  Â  Â  Â  Â  Â  self.data_dir,

Â  Â  Â  Â  Â  Â  target_size=self.img_size,

Â  Â  Â  Â  Â  Â  batch_size=self.batch_size,

Â  Â  Â  Â  Â  Â  class_mode='binary',

Â  Â  Â  Â  Â  Â  subset='training',

Â  Â  Â  Â  Â  Â  shuffle=True,

Â  Â  Â  Â  Â  Â  seed=42

Â  Â  Â  Â  )

Â  Â  Â  Â  

Â  Â  Â  Â  # Validation ì œë„ˆë ˆì´í„°

Â  Â  Â  Â  self.val_generator = val_datagen.flow_from_directory(

Â  Â  Â  Â  Â  Â  self.data_dir,

Â  Â  Â  Â  Â  Â  target_size=self.img_size,

Â  Â  Â  Â  Â  Â  batch_size=self.batch_size,

Â  Â  Â  Â  Â  Â  class_mode='binary',

Â  Â  Â  Â  Â  Â  subset='validation',

Â  Â  Â  Â  Â  Â  shuffle=False,

Â  Â  Â  Â  Â  Â  seed=42

Â  Â  Â  Â  )

Â  Â  Â  Â  

Â  Â  Â  Â  print(f"âœ… Training ìƒ˜í”Œ: {self.train_generator.samples:,}ê°œ")

Â  Â  Â  Â  print(f"âœ… Validation ìƒ˜í”Œ: {self.val_generator.samples:,}ê°œ")

Â  Â  Â  Â  print(f"ğŸ“‹ í´ë˜ìŠ¤ ë§¤í•‘: {self.train_generator.class_indices}")

Â  Â  Â  Â  

Â  Â  Â  Â  return self.train_generator, self.val_generator

Â  Â  

Â  Â  def build_model(self):

Â  Â  Â  Â  """CNN ëª¨ë¸ êµ¬ì¶•"""

Â  Â  Â  Â  print("\nğŸ—ï¸ Â CNN ëª¨ë¸ êµ¬ì¶• ì¤‘...")

Â  Â  Â  Â  

Â  Â  Â  Â  model = models.Sequential([

Â  Â  Â  Â  Â  Â  # ì²« ë²ˆì§¸ Conv Block

Â  Â  Â  Â  Â  Â  layers.Conv2D(32, (3, 3), activation='relu', padding='same', 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â input_shape=(self.img_size[0], self.img_size[1], 3)),

Â  Â  Â  Â  Â  Â  layers.Conv2D(32, (3, 3), activation='relu', padding='same'),

Â  Â  Â  Â  Â  Â  layers.BatchNormalization(),

Â  Â  Â  Â  Â  Â  layers.MaxPooling2D((2, 2)),

Â  Â  Â  Â  Â  Â  layers.Dropout(0.25),

Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  # ë‘ ë²ˆì§¸ Conv Block

Â  Â  Â  Â  Â  Â  layers.Conv2D(64, (3, 3), activation='relu', padding='same'),

Â  Â  Â  Â  Â  Â  layers.Conv2D(64, (3, 3), activation='relu', padding='same'),

Â  Â  Â  Â  Â  Â  layers.BatchNormalization(),

Â  Â  Â  Â  Â  Â  layers.MaxPooling2D((2, 2)),

Â  Â  Â  Â  Â  Â  layers.Dropout(0.25),

Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  # ì„¸ ë²ˆì§¸ Conv Block

Â  Â  Â  Â  Â  Â  layers.Conv2D(128, (3, 3), activation='relu', padding='same'),

Â  Â  Â  Â  Â  Â  layers.Conv2D(128, (3, 3), activation='relu', padding='same'),

Â  Â  Â  Â  Â  Â  layers.BatchNormalization(),

Â  Â  Â  Â  Â  Â  layers.MaxPooling2D((2, 2)),

Â  Â  Â  Â  Â  Â  layers.Dropout(0.25),

Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  # ë„¤ ë²ˆì§¸ Conv Block

Â  Â  Â  Â  Â  Â  layers.Conv2D(256, (3, 3), activation='relu', padding='same'),

Â  Â  Â  Â  Â  Â  layers.Conv2D(256, (3, 3), activation='relu', padding='same'),

Â  Â  Â  Â  Â  Â  layers.BatchNormalization(),

Â  Â  Â  Â  Â  Â  layers.MaxPooling2D((2, 2)),

Â  Â  Â  Â  Â  Â  layers.Dropout(0.25),

Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  # Fully Connected Layers

Â  Â  Â  Â  Â  Â  layers.Flatten(),

Â  Â  Â  Â  Â  Â  layers.Dense(512, activation='relu'),

Â  Â  Â  Â  Â  Â  layers.BatchNormalization(),

Â  Â  Â  Â  Â  Â  layers.Dropout(0.5),

Â  Â  Â  Â  Â  Â  layers.Dense(256, activation='relu'),

Â  Â  Â  Â  Â  Â  layers.BatchNormalization(),

Â  Â  Â  Â  Â  Â  layers.Dropout(0.5),

Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  # Output Layer

Â  Â  Â  Â  Â  Â  layers.Dense(1, activation='sigmoid')

Â  Â  Â  Â  ])

Â  Â  Â  Â  

Â  Â  Â  Â  # ëª¨ë¸ ì»´íŒŒì¼

Â  Â  Â  Â  model.compile(

Â  Â  Â  Â  Â  Â  optimizer=keras.optimizers.Adam(learning_rate=0.001),

Â  Â  Â  Â  Â  Â  loss='binary_crossentropy',

Â  Â  Â  Â  Â  Â  metrics=['accuracy', 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keras.metrics.Precision(name='precision'),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keras.metrics.Recall(name='recall'),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keras.metrics.AUC(name='auc')]

Â  Â  Â  Â  )

Â  Â  Â  Â  

Â  Â  Â  Â  self.model = model

Â  Â  Â  Â  

Â  Â  Â  Â  print("âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")

Â  Â  Â  Â  print(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}ê°œ")

Â  Â  Â  Â  

Â  Â  Â  Â  return model

Â  Â  

Â  Â  def train(self, epochs=50, save_path='models/best_model.h5'):

Â  Â  Â  Â  """ëª¨ë¸ í•™ìŠµ"""

Â  Â  Â  Â  print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")

Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  

Â  Â  Â  Â  # ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° ê³„ì‚°

Â  Â  Â  Â  total_samples = self.train_generator.samples + self.val_generator.samples

Â  Â  Â  Â  print(f"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {total_samples:,}ê°œ")

Â  Â  Â  Â  print(f" Â  - Training: {self.train_generator.samples:,}ê°œ")

Â  Â  Â  Â  print(f" Â  - Validation: {self.val_generator.samples:,}ê°œ")

Â  Â  Â  Â  print(f" Â  - ì˜ˆìƒ í•™ìŠµ ì‹œê°„: {epochs} ì—í¬í¬")

Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  

Â  Â  Â  Â  # ì½œë°± ì„¤ì •

Â  Â  Â  Â  os.makedirs(os.path.dirname(save_path), exist_ok=True)

Â  Â  Â  Â  

Â  Â  Â  Â  # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±

Â  Â  Â  Â  class ProgressCallback(keras.callbacks.Callback):

Â  Â  Â  Â  Â  Â  def __init__(self, total_epochs, total_samples):

Â  Â  Â  Â  Â  Â  Â  Â  self.total_epochs = total_epochs

Â  Â  Â  Â  Â  Â  Â  Â  self.total_samples = total_samples

Â  Â  Â  Â  Â  Â  Â  Â  self.current_epoch = 0

Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  def on_epoch_begin(self, epoch, logs=None):

Â  Â  Â  Â  Â  Â  Â  Â  self.current_epoch = epoch + 1

Â  Â  Â  Â  Â  Â  Â  Â  progress = (self.current_epoch / self.total_epochs) * 100

Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  Â  Â  print(f"\nğŸ“ˆ Epoch {self.current_epoch}/{self.total_epochs} ì‹œì‘")

Â  Â  Â  Â  Â  Â  Â  Â  print(f" Â  ì§„í–‰ë¥ : {progress:.1f}% ({self.current_epoch}/{self.total_epochs})")

Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  Â  Â  # 25% ë‹¨ìœ„ë¡œ í‘œì‹œ

Â  Â  Â  Â  Â  Â  Â  Â  if progress >= 25 and progress < 50:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(" Â  ğŸŸ¡ 25% ì™„ë£Œ - í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤...")

Â  Â  Â  Â  Â  Â  Â  Â  elif progress >= 50 and progress < 75:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(" Â  ğŸŸ  50% ì™„ë£Œ - ì ˆë°˜ì„ ë„˜ì—ˆìŠµë‹ˆë‹¤!")

Â  Â  Â  Â  Â  Â  Â  Â  elif progress >= 75 and progress < 100:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(" Â  ğŸ”´ 75% ì™„ë£Œ - ê±°ì˜ ë‹¤ ì™”ìŠµë‹ˆë‹¤!")

Â  Â  Â  Â  Â  Â  Â  Â  elif progress >= 100:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(" Â  ğŸ‰ 100% ì™„ë£Œ!")

Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  def on_epoch_end(self, epoch, logs=None):

Â  Â  Â  Â  Â  Â  Â  Â  if logs:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  train_acc = logs.get('accuracy', 0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  val_acc = logs.get('val_accuracy', 0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  train_loss = logs.get('loss', 0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  val_loss = logs.get('val_loss', 0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ“Š ê²°ê³¼:")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f" Â  Â  Â Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f" Â  Â  Â Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ì„±ëŠ¥ ê°œì„  í‘œì‹œ

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if epoch > 0:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prev_val_acc = getattr(self, 'prev_val_acc', 0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if val_acc > prev_val_acc:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f" Â  Â  Â ğŸ“ˆ ì„±ëŠ¥ ê°œì„ ! (+{val_acc - prev_val_acc:.4f})")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif val_acc < prev_acc:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f" Â  Â  Â ğŸ“‰ ì„±ëŠ¥ í•˜ë½ (-{prev_val_acc - val_acc:.4f})")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.prev_val_acc = val_acc

Â  Â  Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  Â  Â  print(" Â  " + "-" * 50)

Â  Â  Â  Â  

Â  Â  Â  Â  callbacks = [

Â  Â  Â  Â  Â  Â  ProgressCallback(epochs, total_samples),

Â  Â  Â  Â  Â  Â  EarlyStopping(

Â  Â  Â  Â  Â  Â  Â  Â  monitor='val_loss',

Â  Â  Â  Â  Â  Â  Â  Â  patience=15, Â # ë” ê¸´ patience

Â  Â  Â  Â  Â  Â  Â  Â  restore_best_weights=True,

Â  Â  Â  Â  Â  Â  Â  Â  verbose=1

Â  Â  Â  Â  Â  Â  ),

Â  Â  Â  Â  Â  Â  ModelCheckpoint(

Â  Â  Â  Â  Â  Â  Â  Â  save_path,

Â  Â  Â  Â  Â  Â  Â  Â  monitor='val_accuracy',

Â  Â  Â  Â  Â  Â  Â  Â  save_best_only=True,

Â  Â  Â  Â  Â  Â  Â  Â  verbose=1

Â  Â  Â  Â  Â  Â  ),

Â  Â  Â  Â  Â  Â  ReduceLROnPlateau(

Â  Â  Â  Â  Â  Â  Â  Â  monitor='val_loss',

Â  Â  Â  Â  Â  Â  Â  Â  factor=0.5,

Â  Â  Â  Â  Â  Â  Â  Â  patience=7, Â # ë” ê¸´ patience

Â  Â  Â  Â  Â  Â  Â  Â  min_lr=1e-7,

Â  Â  Â  Â  Â  Â  Â  Â  verbose=1

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  ]

Â  Â  Â  Â  

Â  Â  Â  Â  print(f"\nâ° í•™ìŠµ ì‹œì‘ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")

Â  Â  Â  Â  print("ğŸ’¡ ì¤‘ê°„ì— ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")

Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  

Â  Â  Â  Â  # í•™ìŠµ

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  self.history = self.model.fit(

Â  Â  Â  Â  Â  Â  Â  Â  self.train_generator,

Â  Â  Â  Â  Â  Â  Â  Â  epochs=epochs,

Â  Â  Â  Â  Â  Â  Â  Â  validation_data=self.val_generator,

Â  Â  Â  Â  Â  Â  Â  Â  callbacks=callbacks,

Â  Â  Â  Â  Â  Â  Â  Â  verbose=0 Â # ì»¤ìŠ¤í…€ ì½œë°±ì—ì„œ ì¶œë ¥

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  print("\n" + "=" * 60)

Â  Â  Â  Â  Â  Â  print("âœ… í•™ìŠµ ì™„ë£Œ!")

Â  Â  Â  Â  Â  Â  print(f"â° ì™„ë£Œ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")

Â  Â  Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  except KeyboardInterrupt:

Â  Â  Â  Â  Â  Â  print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

Â  Â  Â  Â  Â  Â  print("ğŸ’¾ í˜„ì¬ê¹Œì§€ì˜ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

Â  Â  Â  Â  Â  Â  return self.history

Â  Â  Â  Â  

Â  Â  Â  Â  return self.history

Â  Â  

Â  Â  def evaluate(self):

Â  Â  Â  Â  """ëª¨ë¸ í‰ê°€"""

Â  Â  Â  Â  print("\nğŸ“Š ëª¨ë¸ í‰ê°€")

Â  Â  Â  Â  print("=" * 60)

Â  Â  Â  Â  

Â  Â  Â  Â  # Validation ë°ì´í„°ë¡œ ì˜ˆì¸¡

Â  Â  Â  Â  val_steps = len(self.val_generator)

Â  Â  Â  Â  y_pred_proba = self.model.predict(self.val_generator, steps=val_steps)

Â  Â  Â  Â  y_pred = (y_pred_proba > 0.5).astype(int).flatten()

Â  Â  Â  Â  y_true = self.val_generator.classes

Â  Â  Â  Â  

Â  Â  Â  Â  # í‰ê°€ ì§€í‘œ ê³„ì‚°

Â  Â  Â  Â  accuracy = accuracy_score(y_true, y_pred)

Â  Â  Â  Â  print(f"\nâœ… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

Â  Â  Â  Â  

Â  Â  Â  Â  print("\nğŸ“‹ Classification Report:")

Â  Â  Â  Â  print(classification_report(y_true, y_pred, 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â target_names=['Down (0)', 'Up (1)']))

Â  Â  Â  Â  

Â  Â  Â  Â  # Confusion Matrix

Â  Â  Â  Â  cm = confusion_matrix(y_true, y_pred)

Â  Â  Â  Â  

Â  Â  Â  Â  return accuracy, cm, y_true, y_pred

Â  Â  

Â  Â  def plot_training_history(self, save_path='results/training_history.png'):

Â  Â  Â  Â  """í•™ìŠµ ê³¼ì • ì‹œê°í™”"""

Â  Â  Â  Â  print("\nğŸ“ˆ í•™ìŠµ ê³¼ì • ì‹œê°í™” ì¤‘...")

Â  Â  Â  Â  

Â  Â  Â  Â  os.makedirs(os.path.dirname(save_path), exist_ok=True)

Â  Â  Â  Â  

Â  Â  Â  Â  fig, axes = plt.subplots(2, 2, figsize=(15, 10))

Â  Â  Â  Â  

Â  Â  Â  Â  # Accuracy

Â  Â  Â  Â  axes[0, 0].plot(self.history.history['accuracy'], label='Train Accuracy')

Â  Â  Â  Â  axes[0, 0].plot(self.history.history['val_accuracy'], label='Val Accuracy')

Â  Â  Â  Â  axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')

Â  Â  Â  Â  axes[0, 0].set_xlabel('Epoch')

Â  Â  Â  Â  axes[0, 0].set_ylabel('Accuracy')

Â  Â  Â  Â  axes[0, 0].legend()

Â  Â  Â  Â  axes[0, 0].grid(True, alpha=0.3)

Â  Â  Â  Â  

Â  Â  Â  Â  # Loss

Â  Â  Â  Â  axes[0, 1].plot(self.history.history['loss'], label='Train Loss')

Â  Â  Â  Â  axes[0, 1].plot(self.history.history['val_loss'], label='Val Loss')

Â  Â  Â  Â  axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')

Â  Â  Â  Â  axes[0, 1].set_xlabel('Epoch')

Â  Â  Â  Â  axes[0, 1].set_ylabel('Loss')

Â  Â  Â  Â  axes[0, 1].legend()

Â  Â  Â  Â  axes[0, 1].grid(True, alpha=0.3)

Â  Â  Â  Â  

Â  Â  Â  Â  # Precision

Â  Â  Â  Â  axes[1, 0].plot(self.history.history['precision'], label='Train Precision')

Â  Â  Â  Â  axes[1, 0].plot(self.history.history['val_precision'], label='Val Precision')

Â  Â  Â  Â  axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')

Â  Â  Â  Â  axes[1, 0].set_xlabel('Epoch')

Â  Â  Â  Â  axes[1, 0].set_ylabel('Precision')

Â  Â  Â  Â  axes[1, 0].legend()

Â  Â  Â  Â  axes[1, 0].grid(True, alpha=0.3)

Â  Â  Â  Â  

Â  Â  Â  Â  # Recall

Â  Â  Â  Â  axes[1, 1].plot(self.history.history['recall'], label='Train Recall')

Â  Â  Â  Â  axes[1, 1].plot(self.history.history['val_recall'], label='Val Recall')

Â  Â  Â  Â  axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')

Â  Â  Â  Â  axes[1, 1].set_xlabel('Epoch')

Â  Â  Â  Â  axes[1, 1].set_ylabel('Recall')

Â  Â  Â  Â  axes[1, 1].legend()

Â  Â  Â  Â  axes[1, 1].grid(True, alpha=0.3)

Â  Â  Â  Â  

Â  Â  Â  Â  plt.tight_layout()

Â  Â  Â  Â  plt.savefig(save_path, dpi=300, bbox_inches='tight')

Â  Â  Â  Â  print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")

Â  Â  Â  Â  plt.close()

Â  Â  

Â  Â  def plot_confusion_matrix(self, cm, save_path='results/confusion_matrix.png'):

Â  Â  Â  Â  """Confusion Matrix ì‹œê°í™”"""

Â  Â  Â  Â  print("\nğŸ“Š Confusion Matrix ì‹œê°í™” ì¤‘...")

Â  Â  Â  Â  

Â  Â  Â  Â  os.makedirs(os.path.dirname(save_path), exist_ok=True)

Â  Â  Â  Â  

Â  Â  Â  Â  plt.figure(figsize=(10, 8))

Â  Â  Â  Â  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â xticklabels=['Down (0)', 'Up (1)'],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â yticklabels=['Down (0)', 'Up (1)'],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â annot_kws={'size': 16})

Â  Â  Â  Â  plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)

Â  Â  Â  Â  plt.ylabel('True Label', fontsize=12)

Â  Â  Â  Â  plt.xlabel('Predicted Label', fontsize=12)

Â  Â  Â  Â  

Â  Â  Â  Â  # ì •í™•ë„ í‘œì‹œ

Â  Â  Â  Â  accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()

Â  Â  Â  Â  plt.text(1, -0.3, f'Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)',

Â  Â  Â  Â  Â  Â  Â  Â  ha='center', fontsize=14, fontweight='bold')

Â  Â  Â  Â  

Â  Â  Â  Â  plt.tight_layout()

Â  Â  Â  Â  plt.savefig(save_path, dpi=300, bbox_inches='tight')

Â  Â  Â  Â  print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")

Â  Â  Â  Â  plt.close()

Â  Â  

Â  Â  def predict_image(self, image_path):

Â  Â  Â  Â  """ë‹¨ì¼ ì´ë¯¸ì§€ ì˜ˆì¸¡"""

Â  Â  Â  Â  # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬

Â  Â  Â  Â  img = Image.open(image_path)

Â  Â  Â  Â  img = img.resize(self.img_size)

Â  Â  Â  Â  img_array = np.array(img) / 255.0

Â  Â  Â  Â  img_array = np.expand_dims(img_array, axis=0)

Â  Â  Â  Â  

Â  Â  Â  Â  # ì˜ˆì¸¡

Â  Â  Â  Â  prediction = self.model.predict(img_array, verbose=0)[0][0]

Â  Â  Â  Â  

Â  Â  Â  Â  result = {

Â  Â  Â  Â  Â  Â  'prediction': 'Up (ìƒìŠ¹)' if prediction > 0.5 else 'Down (í•˜ë½)',

Â  Â  Â  Â  Â  Â  'probability': prediction if prediction > 0.5 else 1 - prediction,

Â  Â  Â  Â  Â  Â  'up_prob': prediction,

Â  Â  Â  Â  Â  Â  'down_prob': 1 - prediction

Â  Â  Â  Â  }

Â  Â  Â  Â  

Â  Â  Â  Â  return result

Â  Â  

Â  Â  def save_model_summary(self, save_path='results/model_summary.txt'):

Â  Â  Â  Â  """ëª¨ë¸ êµ¬ì¡° ì €ì¥"""

Â  Â  Â  Â  os.makedirs(os.path.dirname(save_path), exist_ok=True)

Â  Â  Â  Â  

Â  Â  Â  Â  with open(save_path, 'w', encoding='utf-8') as f:

Â  Â  Â  Â  Â  Â  self.model.summary(print_fn=lambda x: f.write(x + '\n'))

Â  Â  Â  Â  

Â  Â  Â  Â  print(f"âœ… ëª¨ë¸ êµ¬ì¡° ì €ì¥: {save_path}")





def main():

Â  Â  """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

Â  Â  print("\n" + "=" * 60)

Â  Â  print("ğŸš€ ì£¼ì‹ ì°¨íŠ¸ íŒ¨í„´ ê¸°ë°˜ CNN ëª¨ë¸ í”„ë¡œì íŠ¸")

Â  Â  print("ğŸ”¥ ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ ëª¨ë“œ (1,015,729ê°œ ì´ë¯¸ì§€)")

Â  Â  print("=" * 60)

Â  Â  

Â  Â  # 1. ëª¨ë¸ ê°ì²´ ìƒì„± (GPU ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°)

Â  Â  # RTX 4060 8GB VRAMì— ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°

Â  Â  gpus = tf.config.experimental.list_physical_devices('GPU')

Â  Â  batch_size = 128 if gpus else 64 Â # GPU: 128 (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ), CPU: 64

Â  Â  

Â  Â  stock_cnn = StockChartCNN(

Â  Â  Â  Â  data_dir='dataset-2021',

Â  Â  Â  Â  img_size=(100, 100),

Â  Â  Â  Â  batch_size=batch_size

Â  Â  )

Â  Â  

Â  Â  print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size} (GPU: {'ì‚¬ìš©' if gpus else 'ë¯¸ì‚¬ìš©'})")

Â  Â  if gpus:

Â  Â  Â  Â  print("ğŸ’¡ RTX 4060 8GB VRAMì— ìµœì í™”ëœ ì„¤ì •ì…ë‹ˆë‹¤.")

Â  Â  

Â  Â  # 2. ë°ì´í„° íƒìƒ‰

Â  Â  stock_cnn.explore_data()

Â  Â  

Â  Â  # 3. ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±

Â  Â  stock_cnn.create_data_generators(validation_split=0.2)

Â  Â  

Â  Â  # 4. ëª¨ë¸ êµ¬ì¶•

Â  Â  stock_cnn.build_model()

Â  Â  stock_cnn.model.summary()

Â  Â  

Â  Â  # 5. ëª¨ë¸ êµ¬ì¡° ì €ì¥

Â  Â  stock_cnn.save_model_summary()

Â  Â  

Â  Â  # 6. í•™ìŠµ ì‹œì‘ ì „ í™•ì¸

Â  Â  print("\n" + "=" * 60)

Â  Â  print("âš ï¸ Â ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")

Â  Â  print("=" * 60)

Â  Â  print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {stock_cnn.train_generator.samples + stock_cnn.val_generator.samples:,}ê°œ")

Â  Â  print(f"â±ï¸ Â ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-10ì‹œê°„ (GPU) / 50-100ì‹œê°„ (CPU)")

Â  Â  print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ 8-16GB RAM")

Â  Â  print(f"ğŸ–¥ï¸ Â GPU ê¶Œì¥: NVIDIA GPU (CUDA)")

Â  Â  print("=" * 60)

Â  Â  

Â  Â  print("\nğŸš€ ìë™ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")

Â  Â  

Â  Â  # 7. ëª¨ë¸ í•™ìŠµ (ì—í¬í¬ ìˆ˜ ë‹¨ì¶•)

Â  Â  print("\nğŸš€ í•™ìŠµ ì‹œì‘!")

Â  Â  stock_cnn.train(epochs=50, save_path='models/best_stock_chart_model.h5')

Â  Â  

Â  Â  # 8. ëª¨ë¸ í‰ê°€

Â  Â  accuracy, cm, y_true, y_pred = stock_cnn.evaluate()

Â  Â  

Â  Â  # 9. ê²°ê³¼ ì‹œê°í™”

Â  Â  stock_cnn.plot_training_history()

Â  Â  stock_cnn.plot_confusion_matrix(cm)

Â  Â  

Â  Â  # 10. ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸

Â  Â  print("\n" + "=" * 60)

Â  Â  print("ğŸ”® ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")

Â  Â  print("=" * 60)

Â  Â  

Â  Â  # Up ìƒ˜í”Œ

Â  Â  up_sample = os.path.join('dataset-2021/up', os.listdir('dataset-2021/up')[0])

Â  Â  result = stock_cnn.predict_image(up_sample)

Â  Â  print(f"\nğŸ“ˆ Up ìƒ˜í”Œ ì˜ˆì¸¡:")

Â  Â  print(f" Â  íŒŒì¼: {os.path.basename(up_sample)}")

Â  Â  print(f" Â  ì˜ˆì¸¡: {result['prediction']}")

Â  Â  print(f" Â  í™•ë¥ : {result['probability']:.2%}")

Â  Â  print(f" Â  Up í™•ë¥ : {result['up_prob']:.2%}, Down í™•ë¥ : {result['down_prob']:.2%}")

Â  Â  

Â  Â  # Down ìƒ˜í”Œ

Â  Â  down_sample = os.path.join('dataset-2021/down', os.listdir('dataset-2021/down')[0])

Â  Â  result = stock_cnn.predict_image(down_sample)

Â  Â  print(f"\nğŸ“‰ Down ìƒ˜í”Œ ì˜ˆì¸¡:")

Â  Â  print(f" Â  íŒŒì¼: {os.path.basename(down_sample)}")

Â  Â  print(f" Â  ì˜ˆì¸¡: {result['prediction']}")

Â  Â  print(f" Â  í™•ë¥ : {result['probability']:.2%}")

Â  Â  print(f" Â  Up í™•ë¥ : {result['up_prob']:.2%}, Down í™•ë¥ : {result['down_prob']:.2%}")

Â  Â  

Â  Â  print("\n" + "=" * 60)

Â  Â  print("ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ!")

Â  Â  print(f"ğŸ“Š ìµœì¢… ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)")

Â  Â  print("=" * 60)





if __name__ == '__main__':

Â  Â  main()
