"""
ì£¼ì‹ ì°¨íŠ¸ íŒ¨í„´ ê¸°ë°˜ CNN ëª¨ë¸ - ê°œì„  ë²„ì „
- Transfer Learning (ResNet50) ì ìš© + Fine-tuning
- ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•œ Data Augmentation
- Class Weightë¥¼ í†µí•œ ë¶ˆê· í˜• í•´ê²°
- ê°œì„ ëœ í‰ê°€ ì§€í‘œ (ROC-AUC, F1-score)
- 2ë‹¨ê³„ í•™ìŠµ: 1) Transfer Learning (Frozen) â†’ 2) Fine-tuning (Unfreeze)
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import shutil
import random
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,
                             roc_auc_score, roc_curve, f1_score, precision_recall_curve)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import warnings
warnings.filterwarnings('ignore')

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'ignore')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'ignore')

# GPU/CUDA ì„¤ì • ë° ìµœì í™”
print("ğŸ”§ GPU/CUDA ì„¤ì • ë° ìµœì í™” ì¤‘...")
print(f"TensorFlow ë²„ì „: {tf.__version__}")

# GPU ì„¤ì • ê°•ì œ í™œì„±í™”
try:
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(f"ğŸ” ê°ì§€ëœ GPU: {len(gpus)}ê°œ")
    
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        tf.config.experimental.set_visible_devices(gpus, 'GPU')
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {len(gpus)}ê°œ")
        print("ğŸš€ GPU ê°€ì† í•™ìŠµ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤!")
        for i, gpu in enumerate(gpus):
            print(f"   GPU {i}: {gpu.name}")
    else:
        print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        print("ğŸ’¡ CPU ìµœì í™” ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤.")
        tf.config.threading.set_inter_op_parallelism_threads(0)
        tf.config.threading.set_intra_op_parallelism_threads(0)
except Exception as e:
    print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
    print("ğŸ”„ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
    tf.config.threading.set_inter_op_parallelism_threads(0)
    tf.config.threading.set_intra_op_parallelism_threads(0)

# ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)
tf.random.set_seed(42)


def create_subset_dataset(source_dir, target_dir, num_samples_per_class=2500):
    """
    ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ìƒì„±
    
    Args:
        source_dir: ì›ë³¸ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ (e.g., 'dataset-2021')
        target_dir: íƒ€ê²Ÿ ì„œë¸Œì…‹ ë””ë ‰í† ë¦¬ (e.g., 'dataset-subset-5k')
        num_samples_per_class: ê° í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 2500)
    
    Returns:
        target_dir: ìƒì„±ëœ ì„œë¸Œì…‹ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    print("\n" + "="*60)
    print("ğŸ“¦ ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    print("="*60)
    
    # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì‚­ì œ
    if os.path.exists(target_dir):
        print(f"âš ï¸  ê¸°ì¡´ ì„œë¸Œì…‹ ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘: {target_dir}")
        shutil.rmtree(target_dir)
    
    # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(target_dir, exist_ok=True)
    
    classes = ['up', 'down']
    random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
    
    for class_name in classes:
        print(f"\nğŸ“‚ í´ë˜ìŠ¤: {class_name}")
        
        # ì›ë³¸ ë° íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        source_class_dir = os.path.join(source_dir, class_name)
        target_class_dir = os.path.join(target_dir, class_name)
        
        # íƒ€ê²Ÿ í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(target_class_dir, exist_ok=True)
        
        # ì›ë³¸ ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        all_files = [f for f in os.listdir(source_class_dir) 
                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        
        print(f"   ğŸ“Š ì „ì²´ ì´ë¯¸ì§€: {len(all_files):,}ê°œ")
        
        # ëœë¤ ìƒ˜í”Œë§
        if len(all_files) > num_samples_per_class:
            sampled_files = random.sample(all_files, num_samples_per_class)
        else:
            sampled_files = all_files
            print(f"   âš ï¸  ìš”ì²­í•œ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ì ìŒ. ì „ì²´ ì‚¬ìš©: {len(all_files):,}ê°œ")
        
        print(f"   ğŸ¯ ìƒ˜í”Œë§ëœ ì´ë¯¸ì§€: {len(sampled_files):,}ê°œ")
        
        # íŒŒì¼ ë³µì‚¬
        for i, filename in enumerate(sampled_files, 1):
            src_path = os.path.join(source_class_dir, filename)
            dst_path = os.path.join(target_class_dir, filename)
            shutil.copy2(src_path, dst_path)
            
            if i % 500 == 0:
                print(f"   â³ ë³µì‚¬ ì§„í–‰ ì¤‘: {i}/{len(sampled_files)}")
        
        print(f"   âœ… ì™„ë£Œ: {len(sampled_files):,}ê°œ íŒŒì¼ ë³µì‚¬ë¨")
    
    # ê²°ê³¼ ìš”ì•½
    total_samples = sum(len(os.listdir(os.path.join(target_dir, c))) for c in classes)
    print("\n" + "="*60)
    print(f"âœ… ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“‚ ìœ„ì¹˜: {target_dir}")
    print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}ê°œ")
    print(f"   - Up: {len(os.listdir(os.path.join(target_dir, 'up'))):,}ê°œ")
    print(f"   - Down: {len(os.listdir(os.path.join(target_dir, 'down'))):,}ê°œ")
    print("="*60)
    
    return target_dir


class ImprovedStockChartCNN:
    """ê°œì„ ëœ ì£¼ì‹ ì°¨íŠ¸ CNN ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir='dataset-2021', img_size=(100, 100), batch_size=32):
        """
        Args:
            data_dir: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
            img_size: ì´ë¯¸ì§€ í¬ê¸° (width, height)
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.model = None
        self.history = None
        self.class_weights = None
        
    def explore_data(self):
        """ë°ì´í„°ì…‹ íƒìƒ‰ ë° í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        print("=" * 60)
        print("ğŸ“Š ë°ì´í„°ì…‹ íƒìƒ‰")
        print("=" * 60)
        
        up_dir = os.path.join(self.data_dir, 'up')
        down_dir = os.path.join(self.data_dir, 'down')
        
        up_files = os.listdir(up_dir)
        down_files = os.listdir(down_dir)
        
        print(f"âœ… ìƒìŠ¹(Up) ì´ë¯¸ì§€: {len(up_files):,}ê°œ")
        print(f"âŒ í•˜ë½(Down) ì´ë¯¸ì§€: {len(down_files):,}ê°œ")
        print(f"ğŸ“ˆ ì´ ì´ë¯¸ì§€: {len(up_files) + len(down_files):,}ê°œ")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
        total = len(up_files) + len(down_files)
        weight_down = total / (2 * len(down_files))
        weight_up = total / (2 * len(up_files))
        self.class_weights = {0: weight_down, 1: weight_up}
        
        print(f"âš–ï¸  í´ë˜ìŠ¤ ë¹„ìœ¨: Up={len(up_files)/total*100:.1f}%, "
              f"Down={len(down_files)/total*100:.1f}%")
        print(f"ğŸ”§ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: Down={weight_down:.4f}, Up={weight_up:.4f}")
        print("=" * 60)
        
        return len(up_files), len(down_files)
    
    def create_data_generators(self, validation_split=0.2):
        """ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•œ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±"""
        print("\nğŸ”„ ê°œì„ ëœ ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„± ì¤‘...")
        print("ğŸ’¡ ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•œ Augmentation ì ìš©:")
        print("   - ì¢Œìš° ë°˜ì „ ì œê±° (ì‹œê°„ íë¦„ ë³´ì¡´)")
        print("   - ì‘ì€ íšŒì „ë§Œ í—ˆìš© (0-3ë„)")
        print("   - ë…¸ì´ì¦ˆ ë° ë°ê¸° ì¡°ì • ì¶”ê°€")
        
        # Training ë°ì´í„° ì¦ê°• (ê¸ˆìœµ ì°¨íŠ¸ì— ì í•©í•˜ê²Œ)
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=validation_split,
            width_shift_range=0.05,     # ì‘ì€ ì´ë™
            height_shift_range=0.05,    # ì‘ì€ ì´ë™
            horizontal_flip=False,      # ì¢Œìš° ë°˜ì „ ì œê±° (ì‹œê°„ íë¦„ ë³´ì¡´)
            vertical_flip=False,        # ìƒí•˜ ë°˜ì „ ì œê±°
            zoom_range=0.02,            # ì‘ì€ ì¤Œ
            brightness_range=[0.9, 1.1], # ë°ê¸° ì¡°ì •
            fill_mode='nearest'
        )
        
        # Validation ë°ì´í„° (ì¦ê°• ì—†ìŒ)
        val_datagen = ImageDataGenerator(
            rescale=1./255,
            validation_split=validation_split
        )
        
        # Training ì œë„ˆë ˆì´í„° (Grayscale â†’ RGB ë³€í™˜)
        self.train_generator = train_datagen.flow_from_directory(
            self.data_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            subset='training',
            shuffle=True,
            seed=42,
            color_mode='rgb'  # Grayscaleì„ RGBë¡œ ë³€í™˜
        )
        
        # Validation ì œë„ˆë ˆì´í„° (Grayscale â†’ RGB ë³€í™˜)
        self.val_generator = val_datagen.flow_from_directory(
            self.data_dir,
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='binary',
            subset='validation',
            shuffle=False,
            seed=42,
            color_mode='rgb'  # Grayscaleì„ RGBë¡œ ë³€í™˜
        )
        
        print(f"âœ… Training ìƒ˜í”Œ: {self.train_generator.samples:,}ê°œ")
        print(f"âœ… Validation ìƒ˜í”Œ: {self.val_generator.samples:,}ê°œ")
        print(f"ğŸ“‹ í´ë˜ìŠ¤ ë§¤í•‘: {self.train_generator.class_indices}")
        
        return self.train_generator, self.val_generator
    
    def build_model_with_transfer_learning(self):
        """Transfer Learningì„ í™œìš©í•œ ëª¨ë¸ êµ¬ì¶• (ResNet50)"""
        print("\nğŸ—ï¸  Transfer Learning ëª¨ë¸ êµ¬ì¶• ì¤‘ (ResNet50)...")
        print("ğŸ’¡ ImageNet pretrained weights í™œìš©")
        
        # ResNet50 base model (ImageNet pretrained)
        base_model = ResNet50(
            include_top=False,
            weights='imagenet',
            input_shape=(self.img_size[0], self.img_size[1], 3),
            pooling='avg'  # Global Average Pooling
        )
        
        # Base model layersë¥¼ ì²˜ìŒì—ëŠ” freeze (ì „ì´ í•™ìŠµ 1ë‹¨ê³„)
        base_model.trainable = False
        
        print(f"ğŸ“Š ResNet50 Base Model ë¡œë“œ ì™„ë£Œ")
        print(f"   - ì´ ë ˆì´ì–´: {len(base_model.layers)}ê°œ")
        print(f"   - ì´ˆê¸° ìƒíƒœ: Frozen (Transfer Learning)")
        
        # Custom top layers êµ¬ì¶•
        inputs = keras.Input(shape=(self.img_size[0], self.img_size[1], 3))
        x = base_model(inputs, training=False)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        x = layers.Dense(512, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.5)(x)
        x = layers.Dense(256, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.5)(x)
        outputs = layers.Dense(1, activation='sigmoid')(x)
        
        model = keras.Model(inputs, outputs)
        
        # ëª¨ë¸ ì»´íŒŒì¼ (Transfer Learningìš© ë‚®ì€ learning rate)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc'),
                keras.metrics.AUC(name='prc', curve='PR')
            ]
        )
        
        self.model = model
        self.base_model = base_model
        
        print("âœ… Transfer Learning ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        print(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}ê°œ")
        trainable_count = sum([tf.size(w).numpy() for w in model.trainable_weights])
        non_trainable_count = sum([tf.size(w).numpy() for w in model.non_trainable_weights])
        print(f"   - Trainable: {trainable_count:,}ê°œ")
        print(f"   - Non-trainable (Frozen): {non_trainable_count:,}ê°œ")
        
        return model
    
    def unfreeze_base_model(self, unfreeze_layers=50):
        """Base modelì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ë“¤ unfreeze (Fine-tuning)"""
        print(f"\nğŸ”“ Fine-tuning ì‹œì‘: ë§ˆì§€ë§‰ {unfreeze_layers}ê°œ ë ˆì´ì–´ unfreeze...")
        
        # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ trainableë¡œ ì„¤ì •
        self.base_model.trainable = True
        for layer in self.base_model.layers[:-unfreeze_layers]:
            layer.trainable = False
        
        # Fine-tuningì„ ìœ„í•´ ë” ë‚®ì€ learning rateë¡œ ì¬ì»´íŒŒì¼
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.00001),  # 10ë°° ë‚®ì€ learning rate
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc'),
                keras.metrics.AUC(name='prc', curve='PR')
            ]
        )
        
        trainable_count = sum([tf.size(w).numpy() for w in self.model.trainable_weights])
        print(f"âœ… Fine-tuning ì¤€ë¹„ ì™„ë£Œ")
        print(f"ğŸ“Š Trainable íŒŒë¼ë¯¸í„°: {trainable_count:,}ê°œ")
    
    def train(self, epochs=30, save_path='models/improved_model.h5', use_class_weights=True):
        """ê°œì„ ëœ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        
        total_samples = self.train_generator.samples + self.val_generator.samples
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {total_samples:,}ê°œ")
        print(f"   - Training: {self.train_generator.samples:,}ê°œ")
        print(f"   - Validation: {self.val_generator.samples:,}ê°œ")
        
        if use_class_weights:
            print(f"âš–ï¸  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©: {self.class_weights}")
        print("=" * 60)
        
        # ì½œë°± ì„¤ì •
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        callbacks = [
            EarlyStopping(
                monitor='val_auc',
                patience=20,  # 150 ì—í¬í¬ì— ë§ê²Œ ì¦ê°€
                restore_best_weights=True,
                mode='max',
                verbose=1
            ),
            ModelCheckpoint(
                save_path,
                monitor='val_auc',
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=10,  # ë” ê¸´ patience
                min_lr=1e-8,
                verbose=1
            )
        ]
        
        print(f"â° í•™ìŠµ ì‹œì‘ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        # í•™ìŠµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
        try:
            self.history = self.model.fit(
                self.train_generator,
                epochs=epochs,
                validation_data=self.val_generator,
                callbacks=callbacks,
                class_weight=self.class_weights if use_class_weights else None,
                verbose=1
            )
            
            print("\n" + "=" * 60)
            print("âœ… í•™ìŠµ ì™„ë£Œ!")
            print(f"â° ì™„ë£Œ ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 60)
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return self.history
        
        return self.history
    
    def evaluate_comprehensive(self):
        """í¬ê´„ì ì¸ ëª¨ë¸ í‰ê°€"""
        print("\nğŸ“Š í¬ê´„ì ì¸ ëª¨ë¸ í‰ê°€")
        print("=" * 60)
        
        # Validation ë°ì´í„°ë¡œ ì˜ˆì¸¡
        val_steps = len(self.val_generator)
        y_pred_proba = self.model.predict(self.val_generator, steps=val_steps, verbose=1)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        y_true = self.val_generator.classes
        
        # ê¸°ë³¸ í‰ê°€ ì§€í‘œ
        accuracy = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        print(f"\nâœ… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"âœ… F1-Score: {f1:.4f}")
        print(f"âœ… ROC-AUC: {roc_auc:.4f}")
        
        print("\nğŸ“‹ Classification Report:")
        print(classification_report(y_true, y_pred, 
                                   target_names=['Down (0)', 'Up (1)']))
        
        # Confusion Matrix
        cm = confusion_matrix(y_true, y_pred)
        
        # íˆ¬ì ê´€ì  ì§€í‘œ
        print("\nğŸ’° íˆ¬ì ê´€ì  í‰ê°€:")
        tn, fp, fn, tp = cm.ravel()
        
        # Hit Ratio (ì˜ˆì¸¡ ìƒìŠ¹ â†’ ì‹¤ì œ ìƒìŠ¹ ë¹„ìœ¨)
        hit_ratio_up = tp / (tp + fp) if (tp + fp) > 0 else 0
        hit_ratio_down = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        print(f"   ğŸ“ˆ ìƒìŠ¹ ì˜ˆì¸¡ ì ì¤‘ë¥ : {hit_ratio_up:.2%}")
        print(f"   ğŸ“‰ í•˜ë½ ì˜ˆì¸¡ ì ì¤‘ë¥ : {hit_ratio_down:.2%}")
        
        # ì˜ˆì¸¡ ìƒìŠ¹ ì‹œ ì‹¤ì œ ìˆ˜ìµë¥  (ê°€ìƒ)
        print(f"   ğŸ’¡ ìƒìŠ¹ ì˜ˆì¸¡ ì •í™•ë„: {tp}/{tp+fp} = {hit_ratio_up:.2%}")
        print(f"   ğŸ’¡ í•˜ë½ ì˜ˆì¸¡ ì •í™•ë„: {tn}/{tn+fn} = {hit_ratio_down:.2%}")
        
        return accuracy, f1, roc_auc, cm, y_true, y_pred_proba.flatten()
    
    def plot_comprehensive_results(self, y_true, y_pred_proba, 
                                   save_path='results/improved_results.png'):
        """í¬ê´„ì ì¸ ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“ˆ í¬ê´„ì ì¸ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Accuracy
        if self.history and 'accuracy' in self.history.history:
            axes[0, 0].plot(self.history.history['accuracy'], label='Train')
            axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')
            axes[0, 0].set_title('Accuracy', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Loss
        if self.history and 'loss' in self.history.history:
            axes[0, 1].plot(self.history.history['loss'], label='Train')
            axes[0, 1].plot(self.history.history['val_loss'], label='Validation')
            axes[0, 1].set_title('Loss', fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. AUC
        if self.history and 'auc' in self.history.history:
            axes[0, 2].plot(self.history.history['auc'], label='Train AUC')
            axes[0, 2].plot(self.history.history['val_auc'], label='Val AUC')
            axes[0, 2].set_title('ROC-AUC', fontsize=14, fontweight='bold')
            axes[0, 2].set_xlabel('Epoch')
            axes[0, 2].set_ylabel('AUC')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)
        
        # 4. ROC Curve
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        axes[1, 0].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.4f})')
        axes[1, 0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
        axes[1, 0].set_title('ROC Curve', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('False Positive Rate')
        axes[1, 0].set_ylabel('True Positive Rate')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. Precision-Recall Curve
        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
        
        axes[1, 1].plot(recall, precision, linewidth=2, label='PR Curve')
        axes[1, 1].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Recall')
        axes[1, 1].set_ylabel('Precision')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. Prediction Distribution
        axes[1, 2].hist(y_pred_proba[y_true == 0], bins=50, alpha=0.5, label='Down (0)', color='red')
        axes[1, 2].hist(y_pred_proba[y_true == 1], bins=50, alpha=0.5, label='Up (1)', color='blue')
        axes[1, 2].axvline(x=0.5, color='black', linestyle='--', linewidth=2)
        axes[1, 2].set_title('Prediction Distribution', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('Predicted Probability')
        axes[1, 2].set_ylabel('Frequency')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        plt.close()
    
    def plot_confusion_matrix(self, cm, save_path='results/improved_confusion_matrix.png'):
        """Confusion Matrix ì‹œê°í™”"""
        print("\nğŸ“Š Confusion Matrix ì‹œê°í™” ì¤‘...")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
                   xticklabels=['Down (0)', 'Up (1)'],
                   yticklabels=['Down (0)', 'Up (1)'],
                   annot_kws={'size': 16})
        plt.title('Confusion Matrix - Improved Model', fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        
        accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()
        plt.text(1, -0.3, f'Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)',
                ha='center', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        plt.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "=" * 60)
    print("ğŸš€ ê°œì„ ëœ ì£¼ì‹ ì°¨íŠ¸ íŒ¨í„´ CNN ëª¨ë¸")
    print("ğŸ’¡ Transfer Learning (ResNet50) + Fine-tuning")
    print("ğŸ“Š 5,000ì¥ ë°ì´í„°ì…‹ìœ¼ë¡œ íš¨ìœ¨ì  í•™ìŠµ")
    print("=" * 60)
    
    # GPU/CPUì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì„¤ì •
    gpus = tf.config.experimental.list_physical_devices('GPU')
    batch_size = 128 if gpus else 64
    
    # 0. ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ìƒì„± (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    source_dataset = 'dataset-2021'
    target_dataset = 'dataset-subset-5k'
    
    if not os.path.exists(target_dataset):
        print("\nğŸ’¡ ì„œë¸Œì…‹ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        create_subset_dataset(
            source_dir=source_dataset,
            target_dir=target_dataset,
            num_samples_per_class=2500  # ê° í´ë˜ìŠ¤ë‹¹ 2,500ì¥ = ì´ 5,000ì¥
        )
    else:
        print(f"\nâœ… ì„œë¸Œì…‹ ë°ì´í„°ì…‹ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {target_dataset}")
        # ê¸°ì¡´ ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
        up_count = len([f for f in os.listdir(os.path.join(target_dataset, 'up')) 
                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        down_count = len([f for f in os.listdir(os.path.join(target_dataset, 'down')) 
                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        print(f"   ğŸ“Š Up: {up_count:,}ê°œ, Down: {down_count:,}ê°œ (ì´ {up_count + down_count:,}ê°œ)")
    
    # 1. ëª¨ë¸ ê°ì²´ ìƒì„± (ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ì‚¬ìš©)
    stock_cnn = ImprovedStockChartCNN(
        data_dir=target_dataset,  # 5,000ì¥ ì„œë¸Œì…‹ ì‚¬ìš©
        img_size=(100, 100),
        batch_size=batch_size
    )
    
    print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size} (GPU: {'ì‚¬ìš©' if gpus else 'ë¯¸ì‚¬ìš©'})")
    if gpus:
        print("ğŸ’¡ RTX 4060 8GB VRAMì— ìµœì í™”ëœ ì„¤ì •ì…ë‹ˆë‹¤.")
    
    # 2. ë°ì´í„° íƒìƒ‰ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°)
    stock_cnn.explore_data()
    
    # 3. ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
    stock_cnn.create_data_generators(validation_split=0.2)
    
    # 4. Transfer Learning ëª¨ë¸ êµ¬ì¶• (ResNet50)
    stock_cnn.build_model_with_transfer_learning()
    
    # ===== 1ë‹¨ê³„: Transfer Learning (Base model frozen) =====
    print("\n" + "=" * 60)
    print("âš ï¸  1ë‹¨ê³„: Transfer Learning (ResNet50 Frozen)")
    print("=" * 60)
    print("ğŸ’¡ ImageNet íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤")
    print("ğŸš€ ìë™ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 5. 1ë‹¨ê³„ í•™ìŠµ (Base model frozen, 75 ì—í¬í¬)
    print("ğŸ’¡ 5,000ì¥ ë°ì´í„°ì…‹ìœ¼ë¡œ íš¨ìœ¨ì ì¸ í•™ìŠµ ì§„í–‰")
    print("ğŸ’¡ Early Stopping (patience=20)ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€")
    stock_cnn.train(epochs=75, save_path='models/improved_model_stage1.h5')
    
    # ===== 2ë‹¨ê³„: Fine-tuning (Base model ì¼ë¶€ unfreeze) =====
    print("\n" + "=" * 60)
    print("âš ï¸  2ë‹¨ê³„: Fine-tuning (ResNet50 ì¼ë¶€ Unfreeze)")
    print("=" * 60)
    print("ğŸ’¡ ResNet50ì˜ ë§ˆì§€ë§‰ 50ê°œ ë ˆì´ì–´ë¥¼ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤")
    
    # 6. Fine-tuning ì¤€ë¹„
    stock_cnn.unfreeze_base_model(unfreeze_layers=50)
    
    # 7. 2ë‹¨ê³„ í•™ìŠµ (Fine-tuning, 75 ì—í¬í¬)
    stock_cnn.train(epochs=75, save_path='models/improved_model_final.h5')
    
    # 8. í¬ê´„ì ì¸ í‰ê°€
    accuracy, f1, roc_auc, cm, y_true, y_pred_proba = stock_cnn.evaluate_comprehensive()
    
    # 9. ê²°ê³¼ ì‹œê°í™”
    stock_cnn.plot_comprehensive_results(y_true, y_pred_proba)
    stock_cnn.plot_confusion_matrix(cm)
    
    # 10. ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ‰ ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“Š ìµœì¢… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"ğŸ“Š F1-Score: {f1:.4f}")
    print(f"ğŸ“Š ROC-AUC: {roc_auc:.4f}")
    print("=" * 60)
    
    # 11. ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ
    print("\n" + "=" * 60)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ (ê¸°ì¡´ vs ê°œì„ )")
    print("=" * 60)
    print("ê¸°ì¡´ ëª¨ë¸:")
    print("   - Accuracy: 0.5496 (54.96%)")
    print("   - ë‹¨ìˆœ CNN êµ¬ì¡°")
    print("   - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¯¸í•´ê²°")
    print("\nê°œì„ ëœ ëª¨ë¸:")
    print(f"   - Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   - F1-Score: {f1:.4f}")
    print(f"   - ROC-AUC: {roc_auc:.4f}")
    print("   - Transfer Learning (ResNet50) + Fine-tuning")
    print("   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©")
    print("   - ê¸ˆìœµ ì°¨íŠ¸ ìµœì í™” Augmentation")
    
    improvement = ((accuracy - 0.5496) / 0.5496) * 100
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ê°œì„ : {improvement:+.2f}%")
    print("=" * 60)


if __name__ == '__main__':
    main()

